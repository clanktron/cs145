{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Introduction to Data Mining - Assignment 1\n",
    "## Deadline: 12:00PM (noon), April 12, 2024\n",
    "\n",
    "## Instructions\n",
    "Each assignment is structured as a Jupyter notebook, offering interactive tutorials that align with our lectures. You will encounter two types of problems: *write-up problems* and *coding problems*.\n",
    "\n",
    "1. **Write-up Problems:** These problems are primarily theoretical, requiring you to demonstrate your understanding of lecture concepts and to provide mathematical proofs for key theorems. Your answers should include sufficient steps for the mathematical derivations.\n",
    "2. **Coding Problems:** Here, you will be engaging with practical coding tasks. These may involve completing code segments provided in the notebooks or developing models from scratch.\n",
    "\n",
    "To ensure clarity and consistency in your submissions, please adhere to the following guidelines:\n",
    "\n",
    "* For write-up problems, use Markdown bullet points to format text answers. Also, express all mathematical equations using $\\LaTeX$ and avoid plain text such as `x0`, `x^1`, or `R x Q` for equations.\n",
    "* For coding problems, comment on your code thoroughly for readability and ensure your code is executable. Non-runnable code may lead to a loss of **all** points. Coding problems have automated grading, and altering the grading code will result in a deduction of **all** points.\n",
    "* Your submission should show the entire process of data loading, preprocessing, model implementation, training, and result analysis. This can be achieved through a mix of explanatory text cells, inline comments, intermediate result displays, and experimental visualizations.\n",
    "\n",
    "### Collaboration and Integrity\n",
    "\n",
    "* Collaboration is encouraged, but all final submissions must be your own work. Please acknowledge any collaboration or external sources used, including websites, papers, and GitHub repositories.\n",
    "* Any suspicious cases of academic misconduct will be reported to The Office of the Dean of Students.\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "Useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Closed-Form Solution (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset(\"anscombe\")\n",
    "data = data[data.dataset == \"II\"]\n",
    "x = np.array(data['x'])\n",
    "y = np.array(data['y'])\n",
    "x = torch.from_numpy(x).float().unsqueeze(1)\n",
    "y = torch.from_numpy(y).float().unsqueeze(1)\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "# Augment the feature with additional all one dimension\n",
    "X = torch.cat([x_train, torch.ones_like(x_train)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START YOUR CODE HERE\n",
    "# TODO: Compute the closed-form solution for the weights (w) using the normal equation\n",
    "# Hint: Use torch.linalg.inv() for matrix inversion and @ for matrix multiplication\n",
    "w_closed_form = None\n",
    "# END YOUR CODE HERE\n",
    "\n",
    "# Compute training and test error for closed-form solution\n",
    "y_train_pred = x_train @ w_closed_form[0] + w_closed_form[1]\n",
    "y_test_pred = x_test @ w_closed_form[0] + w_closed_form[1]\n",
    "train_error_closed_form = torch.mean((y_train_pred - y_train.reshape(-1))**2).item()\n",
    "test_error_closed_form = torch.mean((y_test_pred - y_test.reshape(-1))**2).item()\n",
    "\n",
    "print(f\"Closed-form solution:\")\n",
    "print(f\"Training error: {train_error_closed_form:.4f}\")\n",
    "print(f\"Test error: {test_error_closed_form:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = sns.load_dataset(\"anscombe\")\n",
    "data = data[data.dataset == \"II\"]\n",
    "\n",
    "x = np.array(data['x'])\n",
    "y = np.array(data['y'])\n",
    "\n",
    "x = torch.from_numpy(x).float().unsqueeze(1)\n",
    "y = torch.from_numpy(y).float().unsqueeze(1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Augment the feature with additional all one dimension\n",
    "X = torch.cat([x_train, torch.ones_like(x_train)], dim=1)\n",
    "\n",
    "# START YOUR CODE HERE\n",
    "# TODO: Compute the closed-form solution for the weights (w) using the normal equation\n",
    "# Hint: Use torch.linalg.inv() for matrix inversion and @ for matrix multiplication\n",
    "w_closed_form = None\n",
    "# END YOUR CODE HERE\n",
    "\n",
    "# Compute training and test error for closed-form solution\n",
    "y_train_pred = x_train @ w_closed_form[0] + w_closed_form[1]\n",
    "y_test_pred = x_test @ w_closed_form[0] + w_closed_form[1]\n",
    "train_error_closed_form = torch.mean((y_train_pred - y_train.reshape(-1))**2).item()\n",
    "test_error_closed_form = torch.mean((y_test_pred - y_test.reshape(-1))**2).item()\n",
    "\n",
    "print(f\"Closed-form solution:\")\n",
    "print(f\"Training error: {train_error_closed_form:.4f}\")\n",
    "print(f\"Test error: {test_error_closed_form:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "x_plot = torch.Tensor(np.linspace(x.min(), x.max(), 1000)).unsqueeze(1)\n",
    "y_plot = x_plot @ w_closed_form[0] + w_closed_form[1]\n",
    "plt.scatter(x_plot, y_plot)\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.scatter(x_test, y_test)\n",
    "plt.title(\"Closed-form solution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Gradient Descent (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.rand(input_dim, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # START YOUR CODE HERE\n",
    "        # TODO: Implement the forward pass of the linear regression model\n",
    "        # Hint: Perform matrix multiplication between the input features (x) and the weights (self.w)\n",
    "        y_pred = None\n",
    "        # END YOUR CODE HERE\n",
    "        return y_pred\n",
    "    \n",
    "    def compute_gradient(self, x, y):\n",
    "        # START YOUR CODE HERE\n",
    "        # TODO: Compute the gradient of the mean squared error loss with respect to the weights (w)\n",
    "        # Hint: Use the chain rule to break down the gradient computation\n",
    "        #       d_loss_d_w = d_loss_d_y * d_y_d_w, where d_loss_d_y is the gradient of the loss w.r.t. the predictions,\n",
    "        #       and d_y_d_w is the gradient of the predictions w.r.t. the weights\n",
    "        d_loss_d_w = None\n",
    "        # END YOUR CODE HERE\n",
    "        return d_loss_d_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the input features\n",
    "x_train_augmented = torch.cat([x_train, torch.ones(len(x_train), 1)], dim=1)\n",
    "x_test_augmented = torch.cat([x_test, torch.ones(len(x_test), 1)], dim=1)\n",
    "\n",
    "input_dim = x_train_augmented.shape[1]\n",
    "model = LinearRegression(input_dim)\n",
    "\n",
    "num_epochs = 2000\n",
    "learning_rate = 0.01\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # START YOUR CODE HERE \n",
    "    # TODO: Compute the gradient for the samples and update the model weights\n",
    "    # Hint: Use model.compute_gradient() to compute the gradients\n",
    "    # END YOUR CODE HERE\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model(x_train_augmented)\n",
    "        y_test_pred = model(x_test_augmented)\n",
    "        train_error = torch.mean((y_train_pred - y_train) ** 2).item()\n",
    "        test_error = torch.mean((y_test_pred - y_test) ** 2).item()\n",
    "        losses.append((train_error, test_error))\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}, training error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train_augmented)\n",
    "    y_test_pred = model(x_test_augmented)\n",
    "    train_error_bgd = torch.mean((y_train_pred - y_train) ** 2).item()\n",
    "    test_error_bgd = torch.mean((y_test_pred - y_test)**2).item()\n",
    "\n",
    "print(f\"\\nBatch Gradient Descent:\")\n",
    "print(f\"Training error: {train_error_bgd:.4f}\")\n",
    "print(f\"Test error: {test_error_bgd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "x_plot = torch.Tensor(np.linspace(x.min(), x.max(), 1000)).unsqueeze(1)\n",
    "y_plot = x_plot @ model.w[0] + model.w[1]\n",
    "y_plot = y_plot.detach().numpy()\n",
    "plt.scatter(x_plot, y_plot)\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.scatter(x_test, y_test)\n",
    "plt.title(\"Linear Regression with Batch Gradient Descent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Stochastic Gradient Descent (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = LinearRegression(input_dim)\n",
    "\n",
    "num_epochs = 1000\n",
    "batch_size = 2\n",
    "learning_rate = 0.001\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        # START YOUR CODE HERE\n",
    "        # TODO: Extract the current batch of training data (x_batch, y_batch)\n",
    "        # Hint: Use indexing on x_train_augmented and y_train\n",
    "        x_batch = None\n",
    "        y_batch = None\n",
    "        # END YOUR CODE HERE\n",
    "        \n",
    "        # START YOUR CODE HERE \n",
    "        # TODO: Compute the gradient for the current batch and update the model weights\n",
    "        # Hint: Use model_sgd.compute_gradient() to compute the gradients\n",
    "        # END YOUR CODE HERE\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_train_pred = model_sgd(x_train_augmented)\n",
    "        y_test_pred = model_sgd(x_test_augmented)\n",
    "        train_error_sgd = torch.mean((y_train_pred - y_train) ** 2).item()\n",
    "        test_error_sgd = torch.mean((y_test_pred - y_test) ** 2).item()\n",
    "        losses.append((train_error_sgd, test_error_sgd))\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch: {epoch}, Train error: {train_error_sgd:.4f}, Test error: {test_error_sgd:.4f}\")\n",
    "\n",
    "print(f\"\\nStochastic Gradient Descent:\")\n",
    "print(f\"Training error: {train_error_sgd:.4f}\")\n",
    "print(f\"Test error: {test_error_sgd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = torch.Tensor(np.linspace(x.min(), x.max(), 1000)).unsqueeze(1)\n",
    "y_plot = x_plot @ model_sgd.w[0] + model_sgd.w[1]\n",
    "y_plot = y_plot.detach().numpy()\n",
    "plt.scatter(x_plot, y_plot)\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.scatter(x_test, y_test)\n",
    "plt.title(\"Linear Regression with Stochastic Gradient Descent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. (10 points) Compare the MSE values on the test dataset for each algorithm. Are they the same? Why or why not?\n",
    "2. (10 points) Apply z-score normalization (which scales the values of a feature to have a mean of 0 and a standard deviation of 1) for the feature and comment whether or not it affect the three algorithms.\n",
    "3. (10 points) Apply a polynomial function to augment the feature and compare the MSE values for batch gradient descent. What do you observe?\n",
    "4. (10 points) Change the learning rate of the stochastic gradient descent algorithm and compare the loss curves. What do you observe?\n",
    "5. (10 points) Ridge regression is adding an $L_2$ regularization term to the original objective function of mean squared error. The objective function become following: \n",
    "    $$ J(\\beta) = \\frac{1}{2n} \\sum_i \\left(x_i^\\mathsf{T}\\beta - y_i \\right)^2 + \\frac{\\lambda}{2n} \\sum_j \\beta_j^2 ,$$ \n",
    "    where $\\lambda \\geq 0$, which is a hyper parameter that controls the trade off. Take the derivative of this provided objective function and derive the closed form solution for $\\beta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
