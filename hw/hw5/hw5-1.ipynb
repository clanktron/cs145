{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Introduction to Data Mining - Assignment 5\n",
    "## Deadline: 11:59PM (midnight), May 17, 2024\n",
    "\n",
    "## Instructions\n",
    "Each assignment is structured as a Jupyter notebook, offering interactive tutorials that align with our lectures. You will encounter two types of problems: *write-up problems* and *coding problems*.\n",
    "\n",
    "1. **Write-up Problems:** These problems are primarily theoretical, requiring you to demonstrate your understanding of lecture concepts and to provide mathematical proofs for key theorems. Your answers should include sufficient steps for the mathematical derivations.\n",
    "2. **Coding Problems:** Here, you will be engaging with practical coding tasks. These may involve completing code segments provided in the notebooks or developing models from scratch.\n",
    "\n",
    "To ensure clarity and consistency in your submissions, please adhere to the following guidelines:\n",
    "\n",
    "* For write-up problems, use Markdown bullet points to format text answers. Also, express all mathematical equations using $\\LaTeX$ and avoid plain text such as `x0`, `x^1`, or `R x Q` for equations.\n",
    "* For coding problems, comment on your code thoroughly for readability and ensure your code is executable. Non-runnable code may lead to a loss of **all** points. Coding problems have automated grading, and altering the grading code will result in a deduction of **all** points.\n",
    "* Your submission should show the entire process of data loading, preprocessing, model implementation, training, and result analysis. This can be achieved through a mix of explanatory text cells, inline comments, intermediate result displays, and experimental visualizations.\n",
    "\n",
    "### Collaboration and Integrity\n",
    "\n",
    "* Collaboration is encouraged, but all final submissions must be your own work. Please acknowledge any collaboration or external sources used, including websites, papers, and GitHub repositories.\n",
    "* Any suspicious cases of academic misconduct will be reported to The Office of the Dean of Students.\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "Useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks that you are allowed to edit (between `START/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU Support\n",
    "\n",
    "Considering the size of the training data, it is strongly suggested to use [Google Colab](https://colab.research.google.com/) or a GPU server for this exercise. If you are using Colab, you can manually switch to a CPU device on Colab by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif USE_GPU and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Variational Autoencoders (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brush-up of Information Theory\n",
    "\n",
    "Many machine learning and deep learning methods are based on a variety of concepts from probability theory and information theory. \n",
    "\n",
    "Since Variational AutoEncoders are very intensive on this aspect, it can be useful to brush up some fundamentals.\n",
    "\n",
    "If you are not already confident with the concepts of marginal probability, Shannon entropy, cross-entropy, Kullback–Leibler divergence and mutual information, we suggest you a fast reading, this [blog post](http://colah.github.io/posts/2015-09-Visual-Information/) with awesome visualizations by Chris Olah; and a book reference, [*Information Theory, Inference, and Learning Algorithms*](https://www.inference.org.uk/itprnn/book.pdf) by David J.C. MacKay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reparameterization trick**\n",
    "\n",
    "We need the reparameterization trick in order to backpropagate through a random node. \n",
    "\n",
    "\n",
    "Given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data\n",
    "data = torch.rand(10)\n",
    "\n",
    "# Some parameters to train\n",
    "deterministic_transform1 = torch.nn.Linear(in_features=10,  out_features=10)\n",
    "deterministic_transform2 = torch.nn.Linear(in_features=10,  out_features=10)\n",
    "\n",
    "# My expected output\n",
    "gold_output = torch.ones(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want optimize the parameters in order to obtain `gold_output` from the following computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My stochastic prediction\n",
    "pred_mean = deterministic_transform1(data)\n",
    "pred_std = torch.ones_like(data)\n",
    "\n",
    "sampling = torch.normal(mean=pred_mean, std=pred_std)     # random numbers drawn from **separate** normal distributions\n",
    "                                                          # whose mean and standard deviation are given.\n",
    "\n",
    "pred_output = deterministic_transform2(sampling)\n",
    "\n",
    "# Error function\n",
    "loss = F.mse_loss(pred_output, gold_output)\n",
    "deterministic_transform1.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# then optimize with some optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)\n",
    "print(deterministic_transform1.weight.grad  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem, this doesn’t work! We don’t have any gradients! They are zero even though the loss is non-zero\n",
    "\n",
    "\n",
    "\n",
    "The specific problem here is that we can’t backpropagate through the random `normal` function. \n",
    "\n",
    "This makes sense right? We are trying to back propagate through a random/stochastic node in the computational graph. \n",
    "It doesn’t make much sense to differentiate with respect to a stochastic node since that means the gradient would technically be a random variable too!\n",
    "\n",
    "So let’s... reparametrize! That is, let’s change how the parameters are incorporated into the model.\n",
    "\n",
    "![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/08/trick.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (10 points)\n",
    "\n",
    "Implement the `reparameterize` function that takes the mean `mu` and log-variance `logvar` as input.\n",
    "In this function, compute the standard deviation from the log-variance using the exponential function.\n",
    "Then, sample a random noise from a standard normal distribution with the same shape as the standard deviation.\n",
    "Finally, compute the sampled latent vector `z` using the reparameterization trick.\n",
    "Return the sampled latent vector `z`.\n",
    "\n",
    "Note that here using the log-variance instead of the standard deviation improves numerical stability during training.\n",
    "The log-variance is used to compute the standard deviation by applying the exponential function: `std = exp(0.5 * logvar)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    \"\"\"\n",
    "    Reparameterization trick to sample from N(mu, var) from N(0, 1).\n",
    "\n",
    "    Args:\n",
    "        mu: Mean of the latent Gaussian [batch_size, latent_dim]\n",
    "        logvar: Log variance of the latent Gaussian [batch_size, latent_dim]\n",
    "\n",
    "    Returns:\n",
    "        z: Sampled latent vector [batch_size, latent_dim]\n",
    "    \"\"\"\n",
    "\n",
    "    eps = torch.randn_like(mu)  # Sample random noise from a standard normal distribution\n",
    "    # START YOUR CODE HERE\n",
    "    std = None  # Compute the standard deviation from the log-variance\n",
    "    z = None  # Compute the sampled latent vector using the reparameterization trick\n",
    "    # END YOUR CODE HERE\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case for reparameterize function\n",
    "mu = torch.tensor([[0.0, 0.0]])\n",
    "logvar = torch.tensor([[1.0, 1.0]])\n",
    "z = reparameterize(mu, logvar)\n",
    "assert z.shape == (1, 2), \"Output shape of reparameterize should be (1, 2)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My stochastic prediction\n",
    "pred_mean = deterministic_transform1(data)\n",
    "pred_std = torch.ones_like(data)\n",
    "\n",
    "rand_source = torch.randn(10)                       # random numbers drawn from **separate** canonical normal distributions\n",
    "sampling = reparameterize(pred_mean, pred_std)\n",
    "\n",
    "pred_output = deterministic_transform2(sampling)\n",
    "\n",
    "# My error\n",
    "loss = F.mse_loss(pred_output, gold_output)\n",
    "deterministic_transform1.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# then optimize with some optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_transform1.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, now we can adjust the parameters of the distribution to improve our stochastic prediction!\n",
    "\n",
    "**We constructed the parameterized random variable via a parameterized deterministic function of a parameter-free random variable.**\n",
    "\n",
    "\n",
    "\n",
    "Another way of thinking about it is that we have moved the source of noise outside of the main flow of the network and used the noise as a way to sample from the expectation.\n",
    "\n",
    "For a formal explanation check out this [post](https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (10 points)\n",
    "\n",
    "Let $x$ be the input, $z$ the latent variable, $θ$ the decoder parameters, and $φ$ the encoder parameters.\n",
    "Show that the VAE training objective $\\log p_\\theta(x)$ consists of two terms:\n",
    "1. Reconstruction loss\n",
    "2. KL divergence between the encoded latent distribution $q_φ(z\\mid x)$ and $p(z)$, the prior distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**\n",
    "\n",
    "The Variational Autoencoder (VAE) training objective aims to maximize the log-likelihood of the observed data $ \\log p_\\theta(x) $. This objective can be decomposed into two terms: the reconstruction loss and the KL divergence between the encoded latent distribution $ q_\\phi(z|x) $ and the prior distribution $ p(z) $. Here's a step-by-step derivation:\n",
    "\n",
    "### VAE Objective: Evidence Lower Bound (ELBO)\n",
    "\n",
    "Given an input $ x $, the log-likelihood $ \\log p_\\theta(x) $ can be decomposed using the variational distribution $ q_\\phi(z|x) $:\n",
    "\n",
    "$ \\log p_\\theta(x) = \\log \\int p_\\theta(x, z) \\, dz $\n",
    "\n",
    "Since the integral is generally intractable, we introduce the variational distribution $ q_\\phi(z|x) $ and use Jensen's inequality to derive a lower bound:\n",
    "\n",
    "$ \\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right] $\n",
    "\n",
    "This lower bound is called the Evidence Lower Bound (ELBO):\n",
    "\n",
    "$ \\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log p_\\theta(x|z) \\right] - \\text{KL}(q_\\phi(z|x) \\| p(z)) $\n",
    "\n",
    "### Decomposition into Reconstruction Loss and KL Divergence\n",
    "\n",
    "Let's break down the terms in the ELBO:\n",
    "\n",
    "1. **Reconstruction Loss**:\n",
    "   $ \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log p_\\theta(x|z) \\right] $\n",
    "\n",
    "   This term represents the expected log-likelihood of the data given the latent variable $ z $. It measures how well the decoder $ p_\\theta(x|z) $ can reconstruct the input $ x $ from the sampled latent variable $ z $.\n",
    "\n",
    "2. **KL Divergence**:\n",
    "   $ \\text{KL}(q_\\phi(z|x) \\| p(z)) = \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log \\frac{q_\\phi(z|x)}{p(z)} \\right] $\n",
    "\n",
    "   This term measures the divergence between the approximate posterior $ q_\\phi(z|x) $ and the prior $ p(z) $. It acts as a regularizer, ensuring that the learned latent distribution does not deviate too far from the prior.\n",
    "\n",
    "### Intuition Behind the Terms\n",
    "\n",
    "- **Reconstruction Loss**: Encourages the model to reconstruct the input $ x $ accurately from the latent variable $ z $. A lower reconstruction loss indicates better reconstruction quality.\n",
    "\n",
    "- **KL Divergence**: Encourages the learned latent distribution $ q_\\phi(z|x) $ to be close to the prior distribution $ p(z) $. This regularization helps to prevent overfitting and ensures that the latent space has a meaningful structure.\n",
    "\n",
    "### Final VAE Objective\n",
    "\n",
    "Combining the two terms, the ELBO can be rewritten as:\n",
    "\n",
    "$ \\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log p_\\theta(x|z) \\right] - \\text{KL}(q_\\phi(z|x) \\| p(z)) $\n",
    "\n",
    "Maximizing this lower bound (ELBO) during training is equivalent to minimizing the reconstruction loss and the KL divergence.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The VAE training objective $ \\log p_\\theta(x) $ consists of two terms:\n",
    "\n",
    "1. **Reconstruction Loss**: $ \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log p_\\theta(x|z) \\right] $, which measures how well the decoder can reconstruct the input.\n",
    "2. **KL Divergence**: $ \\text{KL}(q_\\phi(z|x) \\| p(z)) $, which regularizes the learned latent distribution to be close to the prior distribution.\n",
    "\n",
    "These two terms together ensure that the VAE learns a meaningful latent representation while being able to generate realistic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 (10 points)\n",
    "\n",
    "Instructions:\n",
    "- The `VAE` model class contains an encoder and a decoder.\n",
    "- The encoder outputs the mean `mu` and log-variance `logvar` of the latent Gaussian distribution.\n",
    "- The decoder reconstructs the input data from the sampled latent vector.\n",
    "- Implement the `forward` method to perform the encoding, sampling, and decoding process.\n",
    "- Implement the `loss_function` method to compute the VAE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_channels=1, hidden_channels=64, latent_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=hidden_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_channels, out_channels=hidden_channels*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_channels*2*7*7, latent_dim*2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_channels*2*7*7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (hidden_channels*2, 7, 7)),\n",
    "            nn.ConvTranspose2d(in_channels=hidden_channels*2, out_channels=hidden_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=hidden_channels, out_channels=input_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encodes the input into parameters of a latent Gaussian distribution.\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = x[:, :self.latent_dim], x[:, self.latent_dim:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decodes the latent vector into reconstructed input data.\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # START YOUR CODE HERE\n",
    "        mu, log_var = None  # Encode the input to obtain mean and log-variance of the latent Gaussian\n",
    "        z = None  # Sample the latent vector using the reparameterization trick\n",
    "        recon_x = None  # Decode the latent vector to obtain the reconstructed input\n",
    "        # END YOUR CODE HERE\n",
    "        return recon_x, mu, log_var\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, log_var):\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        \"\"\"\n",
    "        # START YOUR CODE HERE\n",
    "        recon_loss = None  # Reconstruction loss\n",
    "        kl_div = None  # KL divergence loss\n",
    "        loss = None  # Total VAE loss\n",
    "        # END YOUR CODE HERE\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder and Decoder**\n",
    "\n",
    "\n",
    "We'll use a convolutional encoder and decoder, which generally gives better performance than fully connected versions that have the same number of parameters.\n",
    "\n",
    "**Bottleneck**\n",
    "\n",
    "In the convolution layers, we increase the channels as we approach the bottleneck, but note that the total number of features still decreases, since the channels increase by a factor of 2 in each convolution, but the spatial size decreases by a factor of 4.\n",
    "\n",
    "\n",
    "**Deconvolution**\n",
    "\n",
    "In the decoder we are using a transposed convolution, also known as deconvolution. You can think of it as the convolution in the opposite direction.  ([here](https://github.com/vdumoulin/conv_arithmetic) more visualizations)\n",
    "\n",
    "*Blue maps are inputs, and cyan maps are outputs*\n",
    "\n",
    "![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/08/deconv.gif)\n",
    "\n",
    "\n",
    "\n",
    "**Kernel size**\n",
    "\n",
    "We are using `kernel_size=4`. The motivation behind this choice is to lessen the checkerboard artifacts described [here](https://distill.pub/2016/deconv-checkerboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 (10 points)\n",
    "\n",
    "Instructions:\n",
    "- Train the VAE model on the MNIST dataset for a specified number of epochs.\n",
    "- Plot the training loss over epochs.\n",
    "- Visualize the learned latent space by sampling points from the latent space and decoding them.\n",
    "- Analyze the structure of the latent space and discuss your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "dataset = MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = VAE(hidden_channels=64, latent_dim=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        recon_batch, mu, log_var = model(data)\n",
    "        loss = model.loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_history.append(loss.item())\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Sample latent vectors from the prior distribution\n",
    "    prior_samples = torch.randn(64, 2).to('mps')  # Sample 64 points from the prior distribution\n",
    "    generated_samples = model.decode(prior_samples).view(64, 1, 28, 28).detach().cpu()\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(64):\n",
    "        plt.subplot(8, 8, i+1)\n",
    "        plt.imshow(generated_samples[i].squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for _ in range(5):\n",
    "        # Select two random images from the test set\n",
    "        img1, _ = dataset[np.random.randint(len(dataset))]\n",
    "        img2, _ = dataset[np.random.randint(len(dataset))]\n",
    "\n",
    "        # Encode the images into latent vectors\n",
    "        z1 = model.encode(img1.view(1, 1, 28, 28).to(device))[0]\n",
    "        z2 = model.encode(img2.view(1, 1, 28, 28).to(device))[0]\n",
    "\n",
    "        # Interpolate between the latent vectors\n",
    "        interpolations = []\n",
    "        for alpha in np.linspace(0, 1, 20):\n",
    "            z_interp = alpha * z1 + (1 - alpha) * z2\n",
    "            interpolations.append(model.decode(z_interp).view(1, 28, 28))\n",
    "\n",
    "        interpolations = torch.cat(interpolations)\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for i in range(20):\n",
    "            plt.subplot(2, 10, i+1)\n",
    "            plt.imshow(interpolations[i].squeeze().detach().cpu(), cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle('Latent Space Interpolation')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space (from Jeremy Jordan's [blog post](https://www.jeremyjordan.me/variational-autoencoders/))\n",
    "\n",
    "Using VAEs we are able to learn **smooth latent state representations** of the input data. For standard autoencoders, we simply need to learn an encoding which allows us to reproduce the input. \n",
    "\n",
    "As you can see in the left-most figure, focusing only on reconstruction loss does allow us to separate out the classes (in this case, MNIST digits) which should allow our decoder model the ability to reproduce the original handwritten digit, but there's an uneven distribution of data within the latent space. In other words, there are areas in latent space which don't represent any of our observed data.\n",
    "\n",
    "![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/08/latent.png)\n",
    "\n",
    "\n",
    "\n",
    "On the flip side, if we only focus only on ensuring that the latent distribution is similar to the prior distribution (through our KL divergence loss term), we end up describing every observation using the same unit Gaussian, which we subsequently sample from to describe the latent dimensions visualized. This effectively treats every observation as having the same characteristics; in other words, we've failed to describe the original data.\n",
    "\n",
    "However, when the two terms are optimized simultaneously, we're encouraged to describe the latent state for an observation with distributions close to the prior but deviating when necessary to describe salient features of the input.\n",
    "\n",
    "![](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/08/distr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D Latent Space\n",
    "\n",
    "The figure below visualizes the data generated by the decoder network of a variational autoencoder trained on the MNIST handwritten digits dataset. \n",
    "\n",
    "We are doing a linear interpolation over a (multivariate) gaussian distribution!\n",
    "\n",
    "Here, we sample and decode latent space coordinates proportionally to the model’s distribution over latent space. In other words, we simply sample relative to our chosen prior distribution over z\n",
    ". In our case, this means sampling linearly spaced percentiles from the [inverse CDF](http://work.thaslwanter.at/Stats/html/statsDistributions.html#other-important-presentations-of-probability-densities) of a spherical Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "latents_lims = 3.66\n",
    "num_interpolations = 30\n",
    "\n",
    "nd = torch.distributions.Normal(loc=torch.as_tensor([0.]),\n",
    "                                scale=torch.as_tensor([1.]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    latent_interpolation = torch.linspace(0.001, 0.999, num_interpolations)\n",
    "    latent_grid = torch.stack(\n",
    "        (\n",
    "            latent_interpolation.repeat(num_interpolations, 1),\n",
    "            latent_interpolation[:, None].repeat(1, num_interpolations)\n",
    "        ), dim=-1).view(-1, 2)\n",
    "\n",
    "    latent_grid = nd.icdf(latent_grid)\n",
    "    plt.scatter(latent_grid[:,0].cpu().numpy(), latent_grid[:,1].cpu().numpy())\n",
    "    plt.show()\n",
    "    # reconstruct images from the latent vectors\n",
    "    latent_grid = latent_grid.to(device)\n",
    "    image_recon = model.decoder(latent_grid)\n",
    "    image_recon = image_recon.cpu()\n",
    "\n",
    "    # Matplolib plot, much faster for static images\n",
    "    plt.figure(figsize = (17, 17))\n",
    "    plt.imshow(torchvision.utils.make_grid(\n",
    "        image_recon.data[:num_interpolations ** 2], \n",
    "        num_interpolations).permute(1, 2, 0))\n",
    "    plt.title(\"2D latent space\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Part 2: Node Embeddings (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Karate Club Network is a graph describes a social network of 34 members of a karate club and documents links between members who interacted outside the club."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "# G is an undirected graph\n",
    "print(type(G))\n",
    "\n",
    "# Visualize the graph\n",
    "nx.draw(G, with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 (5 points)\n",
    "\n",
    "Compute the average degree of the karate club network.\n",
    "\n",
    "Instructions:\n",
    "- Implement the `average_degree` function that takes the number of edges and nodes as input.\n",
    "- Compute the average degree of the graph.\n",
    "- Round the result to the nearest integer. For example, 3.3 will be rounded to 3 and 3.7 will be rounded to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_degree(num_edges, num_nodes):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_edges: Number of edges in the graph\n",
    "        num_nodes: Number of nodes in the graph\n",
    "\n",
    "    Returns:\n",
    "        avg_degree: Average degree of the graph\n",
    "    \"\"\"\n",
    "    # START YOUR CODE HERE\n",
    "    avg_degree = None  # Compute the average degree\n",
    "    # END YOUR CODE HERE\n",
    "    return avg_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edges = G.number_of_edges()\n",
    "num_nodes = G.number_of_nodes()\n",
    "print(\"Num edges:\", num_edges, \"Num nodeds:\", num_nodes)\n",
    "avg_degree = average_degree(num_edges, num_nodes)\n",
    "print(\"Average degree of karate club network is {}\".format(avg_degree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6 (5 points)\n",
    "\n",
    "Implement functions to compute the Laplacian and Random Walk matrices.\n",
    "\n",
    "Instructions:\n",
    "- Implement the `laplacian_matrix` function that takes the adjacency matrix as input and returns the Laplacian matrix.\n",
    "- Implement the `random_walk_matrix` function that takes the adjacency matrix as input and returns the random walk matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def laplacian_matrix(A):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A: Adjacency matrix of the graph\n",
    "\n",
    "    Returns:\n",
    "        L: Laplacian matrix\n",
    "    \"\"\"\n",
    "    # START YOUR CODE HERE\n",
    "    D = None  # Compute the degree matrix\n",
    "    L = None  # Compute the Laplacian matrix\n",
    "    # END YOUR CODE HERE\n",
    "    return L\n",
    "\n",
    "def random_walk_matrix(A):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A: Adjacency matrix of the graph\n",
    "\n",
    "    Returns:\n",
    "        P: Random walk matrix\n",
    "    \"\"\"\n",
    "    # START YOUR CODE HERE\n",
    "    D = None  # Compute the degree matrix\n",
    "    P = None  # Compute the random walk matrix\n",
    "    # END YOUR CODE HERE\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7 (5 points)\n",
    "\n",
    "Compute the PageRank value for node 0 after one PageRank iteration:\n",
    "\n",
    "$$\n",
    "r_j = \\sum_{i \\rightarrow j} \\beta \\frac{r_i}{d_i} + (1 - \\beta) \\frac{1}{N}\n",
    "$$\n",
    "\n",
    "Instructions:\n",
    "- Implement the `one_iter_pagerank` function that takes the graph, damping factor, initial PageRank value, and node ID as input.\n",
    "- Compute the PageRank value for the specified node after one iteration.\n",
    "- Round the result to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_iter_pagerank(G, beta, r0, node_id):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        beta: Damping factor\n",
    "        r0: Initial PageRank value for each node\n",
    "        node_id: Node to compute PageRank for\n",
    "\n",
    "    Returns:\n",
    "        r1: PageRank value after one iteration\n",
    "    \"\"\"\n",
    "    # START YOUR CODE HERE\n",
    "    r1 = None\n",
    "    # END YOUR CODE HERE\n",
    "    return r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.8\n",
    "r0 = 1 / G.number_of_nodes()\n",
    "node = 0\n",
    "r1 = one_iter_pagerank(G, beta, r0, node)\n",
    "print(\"The PageRank value for node 0 after one iteration is {}\".format(r1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8 (5 points)\n",
    "\n",
    "Compute the final PageRank values using the random walk matrix.\n",
    "\n",
    "Instructions:\n",
    "- Implement the `pagerank` function that takes the random walk matrix, number of iterations, and damping factor as input.\n",
    "- Compute the final PageRank values using the power iteration method.\n",
    "- Compare your results with the PageRank values obtained from NetworkX.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(P, num_iterations=100, d=0.85):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        P: Random walk matrix\n",
    "        num_iterations: Number of iterations\n",
    "        d: Damping factor\n",
    "\n",
    "    Returns:\n",
    "        r: Final PageRank values\n",
    "    \"\"\"\n",
    "    # START YOUR CODE HERE\n",
    "    r = None\n",
    "    # END YOUR CODE HERE\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case for pagerank function\n",
    "G = nx.karate_club_graph()\n",
    "P = random_walk_matrix(nx.adjacency_matrix(G).todense())\n",
    "damping_factor = 0.85\n",
    "num_iterations = 100\n",
    "\n",
    "# Compute PageRank using the implemented pagerank function\n",
    "pagerank_scores = pagerank(P, num_iterations, damping_factor)\n",
    "\n",
    "# Compute PageRank using the NetworkX implementation\n",
    "nx_pagerank_scores = nx.pagerank(G, alpha=damping_factor, max_iter=num_iterations)\n",
    "\n",
    "# Convert NetworkX PageRank scores to a numpy array\n",
    "nx_pagerank_scores = np.array(list(nx_pagerank_scores.values()))\n",
    "\n",
    "print(\"Computed PageRank scores:\")\n",
    "print(pagerank_scores)\n",
    "\n",
    "print(\"NetworkX PageRank scores:\")\n",
    "print(nx_pagerank_scores)\n",
    "\n",
    "# Compare the computed PageRank scores with the NetworkX scores\n",
    "assert np.allclose(pagerank_scores, nx_pagerank_scores, rtol=1e-3), \"Computed PageRank scores do not match the NetworkX scores\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the graph with node sizes proportional to PageRank scores\n",
    "node_sizes_custom = [20000 * pagerank_scores[i] for i in range(len(pagerank_scores))]  # Scale factor for visibility\n",
    "node_sizes_nx = [20000 * nx_pagerank_scores[node] for node in G.nodes()]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(121)\n",
    "nx.draw(G, node_size=node_sizes_custom, with_labels=True, font_weight='bold')\n",
    "plt.title(\"Your PageRank\")\n",
    "\n",
    "plt.subplot(122)\n",
    "nx.draw(G, node_size=node_sizes_nx, with_labels=True, font_color='white', node_color='blue')\n",
    "plt.title(\"NetworkX PageRank\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9 (10 points)\n",
    "Implement negative edge sampling.\n",
    "\n",
    "Instructions:\n",
    "- Implement the `sample_negative_edges` function that takes a NetworkX graph and the number of negative samples as input.\n",
    "- Sample negative edges that do not exist in the graph.\n",
    "- Return the list of sampled negative edges.\n",
    "\n",
    "Notes: You do not need to consider the corner case when the number of possible negative edges\n",
    "is less than `num_neg_samples`. It should be ok as long as your implementation \n",
    "works on the karate club network. In this implementation, self loop should \n",
    "not be considered as either a positive or negative edge. Also, notice that \n",
    "the karate club network is an undirected graph, if (0, 1) is a positive \n",
    "edge, do you think (1, 0) can be a negative one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_edge_list(G):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "\n",
    "    Returns:\n",
    "        edge_list: List of edges in the graph\n",
    "    \"\"\"\n",
    "    edge_list = []\n",
    "    for edge in G.edges():\n",
    "        edge_list.append(edge)  # Append each edge to the edge list\n",
    "    return edge_list\n",
    "\n",
    "def edge_list_to_tensor(edge_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        edge_list: List of edges in the graph\n",
    "\n",
    "    Returns:\n",
    "        edge_index: Tensor of shape (2, num_edges)\n",
    "    \"\"\"\n",
    "    edge_index = torch.tensor(edge_list).t().contiguous()  # Convert the edge list to a tensor and transpose it\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "pos_edge_list = graph_to_edge_list(G)\n",
    "pos_edge_index = edge_list_to_tensor(pos_edge_list)\n",
    "print(\"The pos_edge_index tensor has shape {}\".format(pos_edge_index.shape))\n",
    "print(\"The pos_edge_index tensor has sum value {}\".format(torch.sum(pos_edge_index)))\n",
    "\n",
    "def sample_negative_edges(G, num_neg_samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        num_neg_samples: Number of negative examples to sample\n",
    "\n",
    "    Returns:\n",
    "        neg_edge_list: List of negative edges\n",
    "    \"\"\"\n",
    "    # START YOUR CODE HERE\n",
    "    neg_edge_list = []\n",
    "    # END YOUR CODE HERE\n",
    "    return neg_edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 78 negative edges\n",
    "neg_edge_list = sample_negative_edges(G, len(pos_edge_list))\n",
    "\n",
    "# Transform the negative edge list to tensor\n",
    "neg_edge_index = edge_list_to_tensor(neg_edge_list)\n",
    "print(\"The neg_edge_index tensor has shape {}\".format(neg_edge_index.shape))\n",
    "\n",
    "# Which of following edges can be negative ones?\n",
    "edge_1 = (7, 1)\n",
    "edge_2 = (1, 33)\n",
    "edge_3 = (33, 22)\n",
    "edge_4 = (0, 4)\n",
    "edge_5 = (4, 2)\n",
    "\n",
    "## Note:\n",
    "## 1: For each of the 5 edges, print whether it can be negative edge\n",
    "pos_edge_list = graph_to_edge_list(G)\n",
    "for edge in [edge_1, edge_2, edge_3, edge_4, edge_5]:\n",
    "    edge = (edge[1], edge[0]) if edge[0] > edge[1] else edge\n",
    "    if edge in pos_edge_list:\n",
    "        print(\"No\")\n",
    "    else:\n",
    "        print(\"Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10 (15 points)\n",
    "\n",
    "Train the node embedding model and report the best performance.\n",
    "\n",
    "Instructions:\n",
    "- Implement the `forward` method to compute the dot product of the embeddings for each edge.\n",
    "- train the node embedding model using positive and negative edges.\n",
    "- Use binary cross-entropy loss and optimize the model using Adam optimizer.\n",
    "- Train the model for a specified number of epochs and report the best loss and accuracy achieved.\n",
    "- Plot the training loss over epochs.\n",
    "- Visualize the learned node embeddings in a 2D space using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_nodes, embedding_dim)  # Initialize the embedding layer\n",
    "\n",
    "    def forward(self, edge):\n",
    "        # START YOUR CODE HERE\n",
    "        emb = None  # Get the embeddings for the nodes in the edge\n",
    "        prod = None  # Compute the element-wise product of the embeddings\n",
    "        out = None  # Sum the element-wise product to get the dot product\n",
    "        # END YOUR CODE HERE\n",
    "        return out\n",
    "\n",
    "def train(model, pos_edge_index, neg_edge_index, num_epochs=500):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: Node embedding model\n",
    "        pos_edge_index: Tensor of positive edges\n",
    "        neg_edge_index: Tensor of negative edges\n",
    "        num_epochs: Number of epochs to train for\n",
    "\n",
    "    Returns:\n",
    "        loss_history: List of training losses over epochs\n",
    "        best_loss: Best loss achieved during training\n",
    "        best_accuracy: Best accuracy achieved during training\n",
    "        model: Trained model\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    pos_label = torch.ones(pos_edge_index.shape[1], dtype=torch.float)\n",
    "    neg_label = torch.zeros(neg_edge_index.shape[1], dtype=torch.float)\n",
    "    train_label = torch.cat([pos_label, neg_label], dim=0)\n",
    "    train_edge = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "\n",
    "    loss_history = []\n",
    "    best_loss = float('inf')\n",
    "    best_accuracy = 0.0\n",
    "    loss_fn = nn.BCEWithLogitsLoss()  # Define the binary cross-entropy loss function\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()  # Reset the gradients\n",
    "        out = model(train_edge)  # Forward pass through the model\n",
    "        \n",
    "        loss = loss_fn(out, train_label)  # Compute the loss for the current epoch\n",
    "        loss_history.append(loss.item())  # Append the loss to the list of losses\n",
    "\n",
    "        loss.backward()  # Compute the gradients\n",
    "        optimizer.step()  # Update the model parameters\n",
    "\n",
    "        # Compute the accuracy\n",
    "        pred = torch.sigmoid(out)\n",
    "        pred_label = (pred > 0.5).float()\n",
    "        accuracy = (pred_label == train_label).float().mean()\n",
    "\n",
    "        # Update the best loss and accuracy\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_accuracy = accuracy.item()\n",
    "\n",
    "    return loss_history, best_loss, best_accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "pos_edge_list = graph_to_edge_list(G)\n",
    "pos_edge_index = edge_list_to_tensor(pos_edge_list)\n",
    "neg_edge_list = sample_negative_edges(G, len(pos_edge_list))\n",
    "neg_edge_index = edge_list_to_tensor(neg_edge_list)\n",
    "\n",
    "model = EmbeddingModel(G.number_of_nodes(), embedding_dim=16)\n",
    "loss_history, best_loss, best_accuracy, model = train(model, pos_edge_index, neg_edge_index, num_epochs=500)\n",
    "\n",
    "print(f\"Best loss: {best_loss:.4f}\")\n",
    "print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = model.embedding.weight.data.numpy()\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X)\n",
    "plt.figure(figsize=(6, 6))\n",
    "club1_x = []\n",
    "club1_y = []\n",
    "club2_x = []\n",
    "club2_y = []\n",
    "for node in G.nodes(data=True):\n",
    "    if node[1]['club'] == 'Mr. Hi':\n",
    "        club1_x.append(components[node[0]][0])\n",
    "        club1_y.append(components[node[0]][1])\n",
    "    else:\n",
    "        club2_x.append(components[node[0]][0])\n",
    "        club2_y.append(components[node[0]][1])\n",
    "plt.scatter(club1_x, club1_y, color=\"red\", label=\"Mr. Hi\")\n",
    "plt.scatter(club2_x, club2_y, color=\"blue\", label=\"Officer\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Spectral Clustering (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11 (10 points)\n",
    "Implement symmetric normalized spectral clustering.\n",
    "\n",
    "Instructions:\n",
    "- Implement the `spectral_clustering_sym` function that takes the adjacency matrix and the number of clusters as input.\n",
    "- Compute the symmetric normalized Laplacian matrix.\n",
    "- Compute the eigenvectors corresponding to the smallest eigenvalues of the normalized Laplacian matrix.\n",
    "- Apply K-means clustering on the eigenvectors to obtain the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering_rw(A, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A: Adjacency matrix\n",
    "        k: Number of clusters\n",
    "    Returns:\n",
    "        labels: Cluster labels\n",
    "    \"\"\"\n",
    "    # START YOUR CODE HERE\n",
    "    D = None  # Compute the degree matrix\n",
    "    L = None  # Compute the unnormalized Laplacian matrix\n",
    "    Dinv = None  # Compute the inverse of the degree matrix\n",
    "    Lrw = None  # Compute the random walk normalized Laplacian matrix\n",
    "\n",
    "    # Compute the eigenvectors corresponding to the k smallest eigenvalues\n",
    "    # Apply K-means clustering on the eigenvectors\n",
    "    labels = None # Obtain the cluster labels\n",
    "    # END YOUR CODE HERE\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "A = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "labels_rw = spectral_clustering_rw(A, 4)\n",
    "\n",
    "print(f\"Random walk normalized Laplacian labels: {labels_rw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusterings\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "\n",
    "nx.draw_networkx(G, pos, node_color=labels_rw)\n",
    "plt.title('Random Walk Normalized Laplacian') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
