{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Introduction to Data Mining - Assignment 4\n",
    "## Deadline: 11:59PM (midnight), May 8, 2024\n",
    "\n",
    "## Instructions\n",
    "Each assignment is structured as a Jupyter notebook, offering interactive tutorials that align with our lectures. You will encounter two types of problems: *write-up problems* and *coding problems*.\n",
    "\n",
    "1. **Write-up Problems:** These problems are primarily theoretical, requiring you to demonstrate your understanding of lecture concepts and to provide mathematical proofs for key theorems. Your answers should include sufficient steps for the mathematical derivations.\n",
    "2. **Coding Problems:** Here, you will be engaging with practical coding tasks. These may involve completing code segments provided in the notebooks or developing models from scratch.\n",
    "\n",
    "To ensure clarity and consistency in your submissions, please adhere to the following guidelines:\n",
    "\n",
    "* For write-up problems, use Markdown bullet points to format text answers. Also, express all mathematical equations using $\\LaTeX$ and avoid plain text such as `x0`, `x^1`, or `R x Q` for equations.\n",
    "* For coding problems, comment on your code thoroughly for readability and ensure your code is executable. Non-runnable code may lead to a loss of **all** points. Coding problems have automated grading, and altering the grading code will result in a deduction of **all** points.\n",
    "* Your submission should show the entire process of data loading, preprocessing, model implementation, training, and result analysis. This can be achieved through a mix of explanatory text cells, inline comments, intermediate result displays, and experimental visualizations.\n",
    "\n",
    "### Collaboration and Integrity\n",
    "\n",
    "* Collaboration is encouraged, but all final submissions must be your own work. Please acknowledge any collaboration or external sources used, including websites, papers, and GitHub repositories.\n",
    "* Any suspicious cases of academic misconduct will be reported to The Office of the Dean of Students.\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "Useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks that you are allowed to edit (between `START/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement and explore various clustering models including K-Means and Gaussian Mixture Models (GMM). The assignment consists of coding problems and analysis questions. Follow the instructions for each problem and submit your completed Python file.\n",
    "\n",
    "## Part 1: Theoretical Problems\n",
    "### Problem 1: Nearest Neighbors and the Curse of Dimensionality (8 points)\n",
    "\n",
    "In this question, you will verify the claim that \"most\" points in a high-dimensional space are far away from each other, and also approximately the same distance.\n",
    "\n",
    "1. (4 points) First, consider two independent univariate random variables $X$ and $Y$ sampled uniformly from the unit interval $[0, 1]$. Determine the expectation and variance of the random variable $Z$, defined as the squared distance $Z = (X − Y)^2$.\n",
    "\n",
    "    Hint: If we know $x \\in [a,b] $, then $\\mathbb{E}[X] = \\int_a^b x p(x) \\mathrm{d}x$, where $f(x)$ is probability density function (PDF). The PDF of uniform distribution can be found at [this link](https://en.wikipedia.org/wiki/Continuous_uniform_distribution). Similarly, $\\mathbb{E}[X^2] = \\int_a^b x^2 p(x) \\mathrm{d}x$.\n",
    "\n",
    "\n",
    "2. (4 points) Now suppose we sample two points independently from a unit cube in $d$ dimensions. Observe that each coordinate is sampled independently from $[0, 1]$, i.e. we can view this as sampling random variables $X_1, \\cdots, X_d, Y_1, \\cdots, Y_d$ independently from $[0, 1]$. The squared Euclidean distance can be written as $R = Z_1 + ... + Z_d$, where $Z_i = (X_i − Y_i)^2$. Using the properties of expectation and variance, determine $\\mathbb{E}[R]$ and $\\operatorname{Var}[R]$. You may give your answer in terms of the dimension $d$, and $\\mathbb{E}[Z]$ and $\\operatorname{Var}[Z]$ (the answers from part 1).\n",
    "\n",
    "    Hint: You might find [this](https://online.stat.psu.edu/stat414/lesson/24/24.3) tutorial useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: The Training Error Bound for AdaBoost (12 points)\n",
    "\n",
    "In this problem, you will derive the training error bound for the AdaBoost algorithm.\n",
    "\n",
    "Given:\n",
    "- Training examples $(x_1, y_1), \\ldots, (x_N, y_N)$ where $x_i \\in \\mathcal{X}$ and $y_i \\in \\{-1, +1\\}$.\n",
    "- The AdaBoost algorithm runs for $T$ iterations and produces a final classifier $H(x) = \\text{sign}\\left(\\sum_{t=1}^T \\alpha_t h_t(x)\\right)$, where $h_t$ is the weak classifier trained at iteration $t$ and $\\alpha_t$ is the corresponding weight.\n",
    "- The weighted error of the weak classifier $h_t$ with respect to the distribution $D_t$ at iteration $t$ is given by $\\epsilon_t = \\sum_{i: h_t(x_i) \\neq y_i} D_t(i)$.\n",
    "- The weight update rule for AdaBoost is:\n",
    "\n",
    "$$\n",
    "D_{t+1}(i) = \\frac{D_t(i)}{Z_t} \\times \\begin{cases}\n",
    "  e^{-\\alpha_t} & \\text{if } y_i = h_t(x_i) \\\\\n",
    "  e^{\\alpha_t} & \\text{if } y_i \\neq h_t(x_i)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $Z_t$ is a normalization factor chosen so that $D_{t+1}$ is a valid distribution.\n",
    "\n",
    "Prove that the training error of the final AdaBoost classifier $H$ is bounded by:\n",
    "\n",
    "$$\n",
    "\\text{training error}(H) \\leq \\exp \\left(-2 \\sum_{t=1}^T \\gamma_t^2\\right)\n",
    "$$\n",
    "\n",
    "where $\\epsilon_t = \\frac{1}{2} - \\gamma_t$.\n",
    "\n",
    "**Step 1 (4 points):** Show that:\n",
    "\n",
    "$$\n",
    "D_{T+1}(i)=\\frac{1}{N} \\cdot \\frac{\\exp \\left(-y_i f\\left(x_i\\right)\\right)}{\\prod_t Z_t}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f(x)=\\sum_t \\alpha_t h_t(x) .\n",
    "$$\n",
    "\n",
    "**Step 2 (4 points):** Show that the training error of $H$ is bounded by:\n",
    "\n",
    "$$\n",
    "\\text{training error}(H) \\leq \\prod_t Z_t.\n",
    "$$\n",
    "\n",
    "Hint 1: Use the weight update rule to express $D_{T+1}(i)$ in terms of the exponential loss $\\exp(-y_i f(x_i))$, where $f(x) = \\sum_{t=1}^T \\alpha_t h_t(x)$.\n",
    "\n",
    "Hint 2: $\\exp(-z) \\geq 1$ if $z \\leq 0$.\n",
    "\n",
    "Hint 3: $\\sum_i D(i) =1$ if $D$ is a distribution.\n",
    "\n",
    "**Step 3 (4 points):** Bound the normalization factor $Z_t$ in terms of the weighted error $\\epsilon_t$ and the weight $\\alpha_t$.\n",
    "\n",
    "Hint: Use the inequality $1 + x \\leq \\exp(x)$ for any real $x$ to simplify the bound on $Z_t$.\n",
    "\n",
    "Finally, combine the bounds on the training error and $Z_t$ to obtain the desired result.\n",
    "\n",
    "Please provide a detailed proof of the training error bound for AdaBoost, following the given hints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Part 2: K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "import seaborn as sns\n",
    "\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "transformation = [[1, -1], [-1, 2]]\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c=y, cmap='viridis', edgecolors='k')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "\n",
    "X = np.dot(X, transformation)  # Anisotropic blobs\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c=y, cmap='viridis', edgecolors='k')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Problem 3: L2 Mutual Distances (10 points)\n",
    "\n",
    "In this problem, you will implement functions to calculate the L2 mutual distances between two sets of points.\n",
    "\n",
    "Instructions:\n",
    "- Implement the `l2_mutual_distance` function to calculate the L2 mutual distances between points in `X` and `Y`.\n",
    "- Visualize the similarity matrices using hierarchical clustering with `seaborn.clustermap`.\n",
    "\n",
    "Hints:\n",
    "- An example implementation of `l1_mutual_distance` function to calculate the L1 mutual distances has been provided.\n",
    "- Use `np.expand_dims` to add new dimensions to the arrays for broadcasting.\n",
    "- Use `np.abs` for L1 distance and `np.square` for L2 distance.\n",
    "- Use `np.sum` with `axis=-1` to sum along the last dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_mutual_distance(X, Y):\n",
    "    \"\"\"\n",
    "        X: ndarray [s, ndim]\n",
    "        Y: ndarray [t, ndim]\n",
    "    \"\"\"\n",
    "    X_expand = np.expand_dims(X, axis=1) # X_expand: [s, 1, ndim]\n",
    "    Y_expand = np.expand_dims(Y, axis=0) # Y_expand: [1, t, ndim]\n",
    "    mutual_square = np.abs(X_expand - Y_expand) # [s, t, ndim]\n",
    "    return mutual_square.sum(axis=-1) # [s, t]\n",
    "\n",
    "\n",
    "def l2_mutual_distance(X, Y):\n",
    "    \"\"\"\n",
    "        X: ndarray [s, ndim]\n",
    "        Y: ndarray [t, ndim]\n",
    "    \"\"\"\n",
    "    # START YOUR CODE HERE\n",
    "    X_expand = None # Expand dims of X to [s, 1, ndim]\n",
    "    Y_expand = None # Expand dims of Y to [1, t, ndim]\n",
    "    mutual_square = None # Calculate squared L2 distance between X_expand and Y_expand\n",
    "    return None\n",
    "    # END YOUR CODE HERE\n",
    "\n",
    "# Visualize similarity matrices using hierarchical clustering\n",
    "sim = 1 - l2_mutual_distance(X, X)\n",
    "sns.clustermap(sim)\n",
    "plt.show()\n",
    "\n",
    "sim = 1 - l1_mutual_distance(X, X)\n",
    "sns.clustermap(sim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: K-Means Clustering with L2 Distance (10 points)\n",
    "\n",
    "In this problem, you will implement the K-Means clustering algorithm using L2 distance.\n",
    "\n",
    "Instructions:\n",
    "- Complete the `k_means_clustering` function to perform K-Means clustering on the input data `X` with `k` clusters.\n",
    "- Implement the assignment step to assign each point to the nearest centroid.\n",
    "- Implement the update step to update the centroids based on the assigned points.\n",
    "- Repeat the assignment and update steps until convergence or maximum iterations are reached.\n",
    "- Plot the average distance to cluster centers over iterations.\n",
    "- Apply the implemented K-Means clustering on the provided dataset and visualize the results.\n",
    "\n",
    "Hints:\n",
    "- Use `np.argmin` to find the index of the nearest centroid for each point.\n",
    "- Use `np.mean` to update the centroids based on the assigned points.\n",
    "- Use `np.all` to check for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(X, k, max_iters=100):\n",
    "    # Step 1: Initialize centroids randomly from the data points\n",
    "    indices = np.random.choice(X.shape[0], k, replace=False)\n",
    "    centroids = X[indices]\n",
    "    distance_per_iteration = []\n",
    "\n",
    "    for iter_idx in range(max_iters):\n",
    "        # Step 2: Assignment step\n",
    "        distances = l2_mutual_distance(X, centroids)  # Calculate distances between X and centroids\n",
    "        # START YOUR CODE HERE\n",
    "        closest_cluster = None  # Find the index of the nearest centroid for each point\n",
    "        closest_distance = None  # Get the distance to the nearest centroid for each point\n",
    "        # distance_per_iteration.append(None)  # Append the average distance to the list\n",
    "        # END YOUR CODE HERE\n",
    "\n",
    "        # Step 3: Update step\n",
    "        # START YOUR CODE HERE\n",
    "        new_centroids = None  # Update centroids based on the assigned points\n",
    "        # END YOUR CODE HERE\n",
    "\n",
    "        # Check for convergence (if centroids do not change)\n",
    "        if np.all(centroids == new_centroids):\n",
    "            print('Stop at iter %d' % iter_idx)\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    plt.plot(distance_per_iteration)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Average Distance to Cluster Center')\n",
    "    plt.show()\n",
    "    \n",
    "    return closest_cluster, centroids\n",
    "\n",
    "# Apply K-means to the two-moons dataset with k=2\n",
    "k = 3\n",
    "clusters_kmeans, centroids_kmeans = k_means_clustering(X, k)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clusters_kmeans, cmap='viridis', edgecolors='k')\n",
    "plt.scatter(centroids_kmeans[:, 0], centroids_kmeans[:, 1], c='red', s=200, alpha=0.5)\n",
    "plt.title(\"K-means Clustering with k=%d\" % k)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Additional Questions (15 points)\n",
    "\n",
    "1. Try running K-Means with different values of `n_clusters`. How does the final \"average distance\" change with different `n_clusters`? Why? What if we set `n_clusters = n_points`, what is the lowest average distance in that case?\n",
    "\n",
    "2. Use `from sklearn.metrics import silhouette_samples, silhouette_score` to determine the optimal `n_clusters` for this problem. You can refer to the [scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py) for guidance.\n",
    "\n",
    "3. Try running K-Means with different initialization (random seeds) and record the centroids over 10 rounds. Plot these centroids using `plt.scatter`. Are they the same? Why?\n",
    "\n",
    "For each of the above questions, provide your code and a brief explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: K-Means with L1 Distance (5 points)\n",
    "\n",
    "Suppose the distance metric in K-Means is changed to Manhattan Distance (L1). What is the cost function (J) in this case? By minimizing J, what are the optimal assignment step (E-step) and update step (M-step) now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: Implement K-Means with L1 Distance (10 points)\n",
    "\n",
    "Instructions:\n",
    "- Modify the `k_means_clustering` function from Problem 3 to implement K-Means with L1 distance.\n",
    "- Use the `l1_mutual_distance` function from Problem 2 for distance calculations.\n",
    "- Update the centroids using the median instead of the mean.\n",
    "- Apply the modified K-Means clustering on the provided dataset and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering_l1(X, k, max_iters=100):\n",
    "    # Step 1: Initialize centroids randomly from the data points\n",
    "    indices = np.random.choice(X.shape[0], k, replace=False)\n",
    "    centroids = X[indices]\n",
    "    distance_per_iteration = []\n",
    "    \n",
    "    for iter_idx in range(max_iters):\n",
    "        # Step 2: Assignment step\n",
    "        distances = l1_mutual_distance(X, centroids)\n",
    "        # START YOUR CODE HERE\n",
    "        closest_cluster = None  # Find the index of the nearest centroid for each point\n",
    "        closest_distance = None  # Get the L1 distance to the nearest centroid for each point\n",
    "        distance_per_iteration.append(None)  # Append the average L1 distance to the list\n",
    "        # END YOUR CODE HERE\n",
    "\n",
    "        # Step 3: Update step\n",
    "        # START YOUR CODE HERE\n",
    "        new_centroids = None # Update centroids using the median of assigned points\n",
    "        # END YOUR CODE HERE\n",
    "        \n",
    "        # Check for convergence (if centroids do not change)\n",
    "        if np.all(centroids == new_centroids):\n",
    "            print('Stop at iter %d' % iter_idx)\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    plt.plot(distance_per_iteration)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Average Distance to Cluster Center')\n",
    "    plt.show()\n",
    "    \n",
    "    return closest_cluster, centroids\n",
    "\n",
    "# Apply K-means with L1 distance to the dataset with k=3\n",
    "k = 3\n",
    "clusters_kmeans, centroids_kmeans = k_means_clustering_l1(X, k)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clusters_kmeans, cmap='viridis', edgecolors='k')\n",
    "plt.scatter(centroids_kmeans[:, 0], centroids_kmeans[:, 1], c='red', s=200, alpha=0.5)\n",
    "plt.title(\"K-means Clustering with L1 Distance\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Part 3: Gausian Mixture Models (GMM)\n",
    "\n",
    "In this problem, you will implement the Expectation Maximization (EM) algorithm for Gaussian Mixture Models (GMM) on the dataset.\n",
    "\n",
    "### Problem 8 (10 points)\n",
    "\n",
    "Before implementing the GMM, let's derive some key steps:\n",
    "\n",
    "1. Derive the objective function for the GMM and explain its components.\n",
    "2. Derive the Evidence Lower Bound (ELBO) for the GMM.\n",
    "3. Derive the E-step and M-step update rules for the GMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9 (10 points)\n",
    "\n",
    "In this problem, you will implement the Expectation Maximization (EM) algorithm for Gaussian Mixture Models (GMM) on the dataset.\n",
    "\n",
    "The initialization step to randomly initialize the parameters of the GMM has been given. Please implement the E-step and M-step update rules for the GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.transforms import Affine2D\n",
    "\n",
    "def plot_ellipse(mean, cov, ax, n_std=2.0, facecolor='none', edgecolor='red', **kwargs):\n",
    "    pearson = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, edgecolor=edgecolor, **kwargs)\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    transf = Affine2D().rotate_deg(45).scale(scale_x, scale_y).translate(mean[0], mean[1])\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(X, k):\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Randomly choose k data points as initial means\n",
    "    indices = np.random.choice(n, k, replace=False)\n",
    "    means = X[indices]\n",
    "    \n",
    "    # Initialize covariances to identity matrices\n",
    "    covariances = [np.eye(d) for _ in range(k)]\n",
    "    \n",
    "    # Initialize mixing coefficients uniformly\n",
    "    pis = np.full(k, 1/k)\n",
    "    \n",
    "    return means, covariances, pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(X, means, covariances, pis):\n",
    "    n, k = X.shape[0], len(means)\n",
    "    responsibilities = np.zeros((n, k))\n",
    "    \n",
    "    # Compute responsibilities\n",
    "    for i in range(k):\n",
    "        # Calculate the weighted Gaussian probability density for each data point\n",
    "        # START YOUR CODE HERE\n",
    "        responsibilities[:, i] = None\n",
    "        # END YOUR CODE HERE\n",
    "    \n",
    "    # Normalize the responsibilities to ensure they sum up to 1 for each data point\n",
    "    # Compute the sum of responsibilities for each data point\n",
    "    # START YOUR CODE HERE\n",
    "    responsibilities_sum = None\n",
    "    # END YOUR CODE HERE\n",
    "    \n",
    "    # Normalize responsibilities by dividing each responsibility by the sum of responsibilities for each data point\n",
    "    # START YOUR CODE HERE\n",
    "    # responsibilities = None\n",
    "    # END YOUR CODE HERE\n",
    "    responsibilities /= responsibilities_sum\n",
    "    \n",
    "    # Compute the log-likelihood\n",
    "    # Take the log of the sum of responsibilities for each data point and sum them up\n",
    "    # START YOUR CODE HERE\n",
    "    log_likelihood = None\n",
    "    # END YOUR CODE HERE\n",
    "    \n",
    "    return responsibilities, log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X_test = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "means = np.array([[0.2, 0.8], [0.7, 0.3]])\n",
    "covariances = np.array([[[1.0, 0.1], [0.1, 1.0]], [[0.8, -0.2], [-0.2, 0.8]]])\n",
    "pis = np.array([0.4, 0.6])\n",
    "\n",
    "responsibilities, log_likelihood = e_step(X_test, means, covariances, pis)\n",
    "\n",
    "assert responsibilities.shape == (3, 2), \"Responsibilities should have shape (n, k)\"\n",
    "assert np.allclose(responsibilities.sum(axis=1), 1), \"Responsibilities should sum to 1 for each data point\"\n",
    "\n",
    "expected_responsibilities = np.array(\n",
    "    [[0.36123817, 0.63876183],\n",
    "    [0.34394751, 0.65605249],\n",
    "    [0.34053674, 0.65946326]])\n",
    "\n",
    "expected_likelihood = -5.427135859398128\n",
    "\n",
    "assert np.allclose(responsibilities, expected_responsibilities), \"Responsibilities do not match expected values\"\n",
    "assert np.allclose(log_likelihood, expected_likelihood), \"Log-likelihood does not match expected value\"\n",
    "\n",
    "print(\"e_step test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(X, responsibilities, means, covariances, pis):\n",
    "    n, d = X.shape\n",
    "    k = responsibilities.shape[1]\n",
    "    \n",
    "    # Update parameters\n",
    "    # Calculate the sum of responsibilities for each cluster\n",
    "    Nk = responsibilities.sum(axis=0)\n",
    "    \n",
    "    # Update means\n",
    "    for i in range(k):\n",
    "        # Calculate the responsibility-weighted sum of data points for each cluster and update the mean\n",
    "        # START YOUR CODE HERE\n",
    "        means[i] = None\n",
    "        # END YOUR CODE HERE\n",
    "    \n",
    "    # Update covariances\n",
    "    for i in range(k):\n",
    "        # Calculate the difference between each data point and the updated mean for each cluster\n",
    "        # START YOUR CODE HERE\n",
    "        diff = None\n",
    "        \n",
    "        # Calculate the responsibility-weighted outer product of the differences and update the covariance\n",
    "        covariances[i] = None\n",
    "        # END YOUR CODE HERE\n",
    "    \n",
    "    # Update mixing coefficients based on the sum of responsibilities for each cluster\n",
    "    # START YOUR CODE HERE\n",
    "    pis = None\n",
    "    # END YOUR CODE HERE\n",
    "    \n",
    "    return means, covariances, pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X_test = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "responsibilities = np.array([[0.7, 0.3], [0.4, 0.6], [0.2, 0.8]])\n",
    "means = np.array([[0.2, 0.8], [0.7, 0.3]])\n",
    "covariances = np.array([[[1.0, 0.1], [0.1, 1.0]], [[0.8, -0.2], [-0.2, 0.8]]])\n",
    "pis = np.array([0.4, 0.6])\n",
    "\n",
    "new_means, new_covariances, new_pis = m_step(X_test, responsibilities, means, covariances, pis)\n",
    "\n",
    "assert new_means.shape == means.shape, \"Updated means should have the same shape as original means\"\n",
    "assert len(new_covariances) == len(covariances), \"Updated covariances should have the same length as original covariances\"\n",
    "assert new_pis.shape == pis.shape, \"Updated mixing coefficients should have the same shape as original mixing coefficients\"\n",
    "\n",
    "expected_means = np.array([[0.22307692, 0.32307692], [0.35882353, 0.45882353]])\n",
    "\n",
    "expected_covariances = [\n",
    "    np.array([[0.02177515, 0.02177515],\n",
    "        [0.02177515, 0.02177515]]),\n",
    "    np.array([[0.02242215, 0.02242215],\n",
    "        [0.02242215, 0.02242215]])\n",
    "]\n",
    "expected_pis = np.array([0.43333333, 0.56666667])\n",
    "\n",
    "assert np.allclose(new_means, expected_means), \"Updated means do not match expected values\"\n",
    "for i in range(len(new_covariances)):\n",
    "    assert np.allclose(new_covariances[i], expected_covariances[i]), f\"Updated covariance {i} does not match expected values\"\n",
    "assert np.allclose(new_pis, expected_pis), \"Updated mixing coefficients do not match expected values\"\n",
    "\n",
    "print(\"m_step test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, mu, sigma, clusters):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    for i in range(len(mu)):\n",
    "        zz = multivariate_normal(mean=mu[i], cov=sigma[i]).pdf(np.c_[xx.ravel(), yy.ravel()])\n",
    "        zz = zz.reshape(xx.shape)\n",
    "        plt.contour(xx, yy, zz, levels=5, alpha=0.5)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', marker='o')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_clustering(X, k, max_iters=100, tol=1e-4):\n",
    "    means, covariances, pis = initialize_parameters(X, k)\n",
    "    log_likelihood_old = 0\n",
    "\n",
    "    for ep in range(max_iters):\n",
    "        # E-Step: Compute responsibilities\n",
    "        responsibilities, log_likelihood = e_step(X, means, covariances, pis)\n",
    "        \n",
    "        # M-Step: Update parameters\n",
    "        means, covariances, pis = m_step(X, responsibilities, means, covariances, pis)\n",
    "        \n",
    "        # Check for convergence: compute log likelihood\n",
    "        if np.abs(log_likelihood - log_likelihood_old) < tol:\n",
    "            break\n",
    "        log_likelihood_old = log_likelihood\n",
    "\n",
    "        # Visualize every 2 epochs\n",
    "        if ep % 2 == 0:\n",
    "            plt.title(f\"Iteration {ep}\")\n",
    "            plot_clusters(X, means, covariances, responsibilities.argmax(axis=1))\n",
    "    \n",
    "    return responsibilities.argmax(axis=1), means, covariances, pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GMM to the dataset with k=3\n",
    "k = 3\n",
    "clusters_gmm, means_gmm, covariances_gmm, pis_gmm = gmm_clustering(X, k)\n",
    "\n",
    "plt.title(\"Final Results Gaussian Mixture Model Clustering\")\n",
    "plot_clusters(X, means_gmm, covariances_gmm, clusters_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10 (10 points)\n",
    "\n",
    "Now assume we force the covariance matrices of a GMM model are fixed to be diagonal (https://en.wikipedia.org/wiki/Diagonal_matrix). \n",
    "\n",
    "Derive the updated E-step and M-step for this case. Copy your implementation from Problem 9 and implement the corresponding changes in the code. \n",
    "\n",
    "Compare the final cWhy does the shape of the decision boundary change after fixing the covariance matrices to be diagonal?\n",
    "(To check your output shape is correct, look at: https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_diag(X, k):\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # Randomly choose k data points as initial means\n",
    "    indices = np.random.choice(n, k, replace=False)\n",
    "    means = X[indices]\n",
    "    \n",
    "    # Initialize covariances to diagonal matrices\n",
    "    covariances = [np.eye(d) for _ in range(k)]\n",
    "    \n",
    "    # Initialize mixing coefficients uniformly\n",
    "    pis = np.full(k, 1/k)\n",
    "    \n",
    "    return means, covariances, pis\n",
    "\n",
    "def e_step_diag(X, means, covariances, pis):\n",
    "    n, k = X.shape[0], len(means)\n",
    "    responsibilities = np.zeros((n, k))\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Compute the Gaussian probability density for each data point\n",
    "        # TODO\n",
    "        \n",
    "    # Normalize responsibilities\n",
    "    \n",
    "    # Compute the log-likelihood\n",
    "    \n",
    "    return responsibilities, log_likelihood\n",
    "\n",
    "def m_step_diag(X, responsibilities, means, covariances, pis):\n",
    "    n, d = X.shape\n",
    "    k = responsibilities.shape[1]\n",
    "    \n",
    "    # Update mixing coefficients\n",
    "    \n",
    "    # Update means\n",
    "    \n",
    "    # Update diagonal covariances\n",
    "    \n",
    "    return means, covariances, pis\n",
    "\n",
    "def gmm_clustering_diag(X, k, max_iters=100, tol=1e-4):\n",
    "    means, covariances, pis = initialize_parameters_diag(X, k)\n",
    "    log_likelihood_old = 0\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # E-step\n",
    "        responsibilities, log_likelihood = e_step_diag(X, means, covariances, pis)\n",
    "        \n",
    "        # M-step\n",
    "        means, covariances, pis = m_step_diag(X, responsibilities, means, covariances, pis)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.abs(log_likelihood - log_likelihood_old) < tol:\n",
    "            break\n",
    "        log_likelihood_old = log_likelihood\n",
    "\n",
    "        # Visualize every 10 epochs\n",
    "        if i % 10 == 0:\n",
    "            plt.title(f\"Iteration {i}\")\n",
    "            plot_clusters(X, means, covariances, responsibilities.argmax(axis=1))\n",
    "    \n",
    "    clusters = responsibilities.argmax(axis=1)\n",
    "    \n",
    "    return clusters, means, covariances, pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GMM to the dataset with k=3\n",
    "k = 3\n",
    "clusters_gmm, means_gmm, covariances_gmm, pis_gmm = gmm_clustering_diag(X, k)\n",
    "\n",
    "plt.title(\"Final Results Gaussian Mixture Model Clustering with Diagonal Covariance\")\n",
    "plot_clusters(X, means_gmm, covariances_gmm, clusters_gmm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
