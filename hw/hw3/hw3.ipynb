{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Introduction to Data Mining - Assignment 3\n",
    "## Deadline: 11:59PM (midnight), April 28, 2024\n",
    "\n",
    "## Instructions\n",
    "Each assignment is structured as a Jupyter notebook, offering interactive tutorials that align with our lectures. You will encounter two types of problems: *write-up problems* and *coding problems*.\n",
    "\n",
    "1. **Write-up Problems:** These problems are primarily theoretical, requiring you to demonstrate your understanding of lecture concepts and to provide mathematical proofs for key theorems. Your answers should include sufficient steps for the mathematical derivations.\n",
    "2. **Coding Problems:** Here, you will be engaging with practical coding tasks. These may involve completing code segments provided in the notebooks or developing models from scratch.\n",
    "\n",
    "To ensure clarity and consistency in your submissions, please adhere to the following guidelines:\n",
    "\n",
    "* For write-up problems, use Markdown bullet points to format text answers. Also, express all mathematical equations using $\\LaTeX$ and avoid plain text such as `x0`, `x^1`, or `R x Q` for equations.\n",
    "* For coding problems, comment on your code thoroughly for readability and ensure your code is executable. Non-runnable code may lead to a loss of **all** points. Coding problems have automated grading, and altering the grading code will result in a deduction of **all** points.\n",
    "* Your submission should show the entire process of data loading, preprocessing, model implementation, training, and result analysis. This can be achieved through a mix of explanatory text cells, inline comments, intermediate result displays, and experimental visualizations.\n",
    "\n",
    "### Collaboration and Integrity\n",
    "\n",
    "* Collaboration is encouraged, but all final submissions must be your own work. Please acknowledge any collaboration or external sources used, including websites, papers, and GitHub repositories.\n",
    "* Any suspicious cases of academic misconduct will be reported to The Office of the Dean of Students.\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "Useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Appendix**\n",
    "\n",
    "Here, we give some background on convexity which you may find useful for some of the questions in this assignment. You may assume anything given here.\n",
    "\n",
    "Convexity is an important concept in mathematics with many uses in machine learning. We briefly define convex set and function and some of their properties here. Using these properties are useful in solving some of the questions in the rest of this homework. If you are interested to know more about convexity, refer to Boyd and Vandenberghe, Convex Optimization, 2004.\n",
    "\n",
    "A set $C$ is convex if the line segment between any two points in $C$ lies within $C$, i.e., if for any $x_1, x_2 \\in C$ and for any $0 \\leq \\lambda \\leq 1$, we have\n",
    "$$\n",
    "\\lambda x_1+(1-\\lambda) x_2 \\in C .\n",
    "$$\n",
    "\n",
    "For example, a cube or sphere in $\\mathbb{R}^d$ are convex sets, but a cross (a shape like $\\mathrm{X}$ ) is not.\n",
    "A function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is convex if its domain is a convex set and if for all $x_1, x_2$ in its domain, and for any $0 \\leq \\lambda \\leq 1$, we have\n",
    "$$\n",
    "f\\left(\\lambda x_1+(1-\\lambda) x_2\\right) \\leq \\lambda f\\left(x_1\\right)+(1-\\lambda) f\\left(x_2\\right) .\n",
    "$$\n",
    "\n",
    "This inequality means that the line segment between $\\left(x_1, f\\left(x_1\\right)\\right)$ and $\\left(x_2, f\\left(x_2\\right)\\right)$ lies above the graph of $f$. A convex function looks like $\\smile$. We say that $f$ is concave if $-f$ is convex. A concave function looks like $\\frown$.\n",
    "\n",
    "Some examples of convex and concave functions are:\n",
    "- Powers: $x^p$ is convex on the set of positive real numbers when $p \\geq 1$ or $p \\leq 0$. It is concave for $0 \\leq p \\leq 1$.\n",
    "- Exponential: $e^{a x}$ is convex on $\\mathbb{R}$, for any $a \\in \\mathbb{R}$.\n",
    "- Logarithm: $\\log (x)$ is concave on the set of positive real numbers.\n",
    "- Norms: Every norm on $\\mathbb{R}^d$ is convex.\n",
    "- Max function: $f(x)=\\max \\left\\{x_1, x_2, \\ldots, x_d\\right\\}$ is convex on $\\mathbb{R}^d$.\n",
    "- Log-sum-exp: The function $f(x)=\\log \\left(e^{x_1}+\\ldots+e^{x_d}\\right)$ is convex on $\\mathbb{R}^d$.\n",
    "\n",
    "\n",
    "An important property of convex and concave functions, which you may need to use in your homework, is <mark>Jensen's inequality</mark>. Jensen's inequality states that if $\\phi(x)$ is a convex function of $x$, we have\n",
    "$$\n",
    "\\phi(\\mathbb{E}[X]) \\leq \\mathbb{E}[\\phi(X)] .\n",
    "$$\n",
    "\n",
    "In words, if we apply a convex function to the expectation of a random variable, it is less than or equal to the expected value of that convex function when its argument is the random variable. If the function is concave, the direction of the inequality is reversed.\n",
    "\n",
    "Jensen's inequality has a physical interpretation: Consider a set $\\mathcal{X}=\\left\\{x_1, \\ldots, x_N\\right\\}$ of points on $\\mathbb{R}$. Corresponding to each point, we have a probability $p\\left(x_i\\right)$. If we interpret the probability as mass, and we put an object with mass $p\\left(x_i\\right)$ at location $\\left(x_i, \\phi\\left(x_i\\right)\\right)$, then the centre of gravity of these objects, which is in $\\mathbb{R}^2$, is located at the point $(\\mathbb{E}[X], \\mathbb{E}[\\phi(X)])$. If $\\phi$ is convex $\\smile$, the centre of gravity lies above the curve $x \\mapsto \\phi(x)$, and vice versa for a concave function $\\frown$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Theoretical Problems\n",
    "\n",
    "### Problem 1 (Information Theory, 16 points)\n",
    "\n",
    "The goal of this question is to help you become more familiar with the basic equalities and inequalities of information theory. They appear in many contexts in machine learning and elsewhere, so having some experience with them is quite helpful. We review some concepts from information theory, and ask you a few questions.\n",
    "\n",
    "Recall the definition of the entropy of a discrete random variable $X$ with probability mass function $p: H(X)=\\sum_x p(x) \\log _2\\left(\\frac{1}{p(x)}\\right)$. Here the summation is over all possible values of $x \\in \\mathcal{X}$, which (for simplicity) we assume is finite. For example, $\\mathcal{X}$ might be $\\{1,2, \\ldots, N\\}$. You may assume $p(x) \\log _2\\left(\\frac{1}{p(x)}\\right)=0$ if $p(x)=0$.\n",
    "\n",
    "1. (4 points) Prove that the entropy $H(X)$ is non-negative.\n",
    "\n",
    "    **Answer:**\n",
    "    We already know that $ 0 \\le p(x) \\le 1$. This would also imply that $\\log _2 (1/p(x))$ is $\\ge 1$. Since both of these values are non-negative, $p(x)\\log _2 (1/p(x))$ can be stated to be always positive. The sum of only positive values cannot result in a negative value, therefore $H(X)$ is non-negative.\n",
    "\n",
    "2. (4 points) Show the relationship between cross-entropy loss and KL divergence: $H(p, q) = H(p) + \\mathrm{KL}(p \\| q)$:\n",
    "\n",
    "    <mark> KL-divergence</mark>  (or called relative entropy) of two distributions $p$ and $q$ is defined as\n",
    "    $$\n",
    "    \\mathrm{KL}(p \\| q)=\\sum_x p(x) \\log _2 \\frac{p(x)}{q(x)} .\n",
    "    $$\n",
    "\n",
    "    The KL-divergence is one of the most commonly used measure of difference (or divergence) between two distributions, and it regularly appears in information theory, machine learning, and statistics. For this question, you may assume $p(x)>0$ and $q(x)>0$ for all $x$.\n",
    "    If two distributions are close to each other, their KL divergence is small. If they are exactly the same, their KL divergence is zero. KL divergence is not a true distance metric (since it isn't symmetric and doesn't satisfy the triangle inequality), but we often use it as a measure of dissimilarity between two probability distributions.\n",
    "    \n",
    "    <mark>Cross-entropy</mark> between two probability distributions $p$ and $q$ is defined as\n",
    "    $$\n",
    "    H(p, q) = -\\sum_x p(x) \\log_2 q(x).\n",
    "    $$\n",
    "    Prove that the cross-entropy can be expressed as the sum of the entropy of $p$ and the KL divergence between $p$ and $q$, i.e.,\n",
    "    $$\n",
    "    H(p, q) = H(p) + \\mathrm{KL}(p \\| q).\n",
    "    $$\n",
    "\n",
    "    **Answer:**\n",
    "    $$ H(p,q) = \\sum_x p(x) \\log _2 (1/p(x)) + \\sum_x p(x) \\log _2 (p(x)/q(x)) $$\n",
    "    $$ H(p,q) = \\sum_x p(x) \\log _2 (1/p(x)) + p(x) \\log _2 (p(x)/q(x)) $$\n",
    "    Factor:\n",
    "    $$ H(p,q) = \\sum_x p(x) (\\log _2 (1/p(x)) + \\log _2 (p(x)/q(x))) $$\n",
    "    Log properties:\n",
    "    $$ H(p,q) = \\sum_x p(x) (\\log _2 (1/q(x))) $$\n",
    "    $$ H(p,q) = - \\sum_x p(x) (\\log _2 (q(x))) $$\n",
    "\n",
    "3. (4 points) Prove that $\\mathrm{KL}(p \\| q)$ is non-negative. <mark> Hint: You might use Jensen's inequality in appendix above</mark>.\n",
    "\n",
    "    **Answer:**\n",
    "\n",
    "    - using Jensen's inequality\n",
    "    - since -log(x) is a strictly convex function for $x\\ge0$\n",
    "\n",
    "    $$ \\sum_x p(x) (\\log _2 (p(x)/q(x))) \\ge \\log _2( \\sum_x p(x) * p(x)/q(x) )$$\n",
    "    $$\\log (\\sum_x q(x))$$\n",
    "\n",
    "    - and since all $ q(x) \\ge 0$ this is always positive\n",
    "    \n",
    "4. (4 points) The Information Gain or Mutual Information between $X$ and $Y$ is $I(Y ; X)=$ $H(Y)-H(Y \\mid X)$. Show that\n",
    "    $$\n",
    "    I(Y ; X)=\\mathrm{KL}(p(x, y) \\| p(x) p(y)),\n",
    "    $$\n",
    "    where $p(x)=\\sum_y p(x, y)$ is the marginal distribution of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (Benefit of Averaging, 4 points)\n",
    "\n",
    "Consider $m$ estimators $h_1, \\ldots, h_m$, each of which accepts an input $x$ and produces an output $y$, i.e., $y_i=h_i(x)$. These estimators might be generated through a Bagging procedure, but that is not necessary to the result that we want to prove. Consider the squared error loss function $L(y, t)=\\frac{1}{2}(y-t)^2$. Show that the loss of the average estimator\n",
    "$$\n",
    "\\bar{h}(x)=\\frac{1}{m} \\sum_{i=1}^m h_i(x),\n",
    "$$\n",
    "is smaller than the average loss of the estimators. That is, for any $x$ and $t$, we have\n",
    "$$\n",
    "L(\\bar{h}(x), t) \\leq \\frac{1}{m} \\sum_{i=1}^m L\\left(h_i(x), t\\right) .\n",
    "$$\n",
    "\n",
    "<mark> Hint: You might use Jensen's inequality in appendix above</mark>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install graphviz numpy scikit-learn seaborn torch sympy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import graphviz\n",
    "from typing import Optional, List, Any, Tuple\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 (10 points)\n",
    "Implement the `gini` function. The `gini` function should return the Gini impurity of a classification problem. It should take a list of class labels as input and return a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a list of class labels.\n",
    "\n",
    "    Parameters:\n",
    "    y (List[int]): List of class labels.\n",
    "\n",
    "    Returns:\n",
    "    float: Entropy.\n",
    "    \"\"\"\n",
    "    # Calculate the proportions of each class label in the target variable y\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / counts.sum()\n",
    "    # Calculate the entropy using the proportions\n",
    "    probabilities = probabilities.clip(1e-10, 1-1e-10)\n",
    "    # The entropy is the negative sum of p * log2(p) for each non-zero proportion p\n",
    "    ent = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return ent\n",
    "\n",
    "def gini(y: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Gini impurity of a list of class labels.\n",
    "\n",
    "    Parameters:\n",
    "    y (List[int]): List of class labels.\n",
    "\n",
    "    Returns:\n",
    "    float: Gini impurity.\n",
    "    \"\"\"\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / counts.sum()\n",
    "    gini_impurity = 1 - sum(probabilities ** 2)\n",
    "    return gini_impurity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "y = [0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n",
    "assert np.isclose(gini(y), 0.6446, atol=1e-4), \"Gini impurity test case failed.\"\n",
    "assert np.isclose(entropy(y), 1.5395, atol=1e-4), \"Entropy test case failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 (10 points)\n",
    "\n",
    "Implement the `_calculate_gain` function in the `MyDecisionTreeClassifier` class.\n",
    "\n",
    "- Calculate the specified gain (information gain or Gini gain) for a given split.\n",
    "- Use the `entropy` or `gini` function to calculate the impurity of the parent and child nodes.\n",
    "- Return the gain, which is the reduction in impurity.\n",
    "Hint: Be careful with division by zero when calculating the weighted average of child impurities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 (10 points)\n",
    "\n",
    "Implement the `_build_tree` function in the `MyDecisionTreeClassifier` class.\n",
    "\n",
    "- Complete the `_build_tree` function to build the decision tree recursively.\n",
    "- Use the `_best_split` function to find the best split for each node.\n",
    "- Create left and right subtrees by calling _build_tree recursively.\n",
    "\n",
    "Hint: You can use the `_split` function to split the data based on the best split.\n",
    "\n",
    "\n",
    "Also explain in our implementation, how we calculate feature importance and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature: Optional[int] = None, threshold: Optional[float] = None,\n",
    "                 left: 'Optional[Node]' = None, right: 'Optional[Node]' = None, *,\n",
    "                 value: Optional[Any] = None, feature_name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize a decision tree node.\n",
    "\n",
    "        Parameters:\n",
    "            feature (Optional[int]): The index of the feature used for splitting.\n",
    "            threshold (Optional[float]): The threshold value at which the split is made.\n",
    "            left (Optional[Node]): The left child node.\n",
    "            right (Optional[Node]): The right child node.\n",
    "            value (Optional[Any]): The class label if the node is a leaf.\n",
    "            feature_name (Optional[str]): The name of the feature used for splitting.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        self.feature_name = feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import left\n",
    "\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self, gain_name: str, feature_names: List[str], max_depth: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Constructor for the MyDecisionTreeClassifier.\n",
    "\n",
    "        Parameters:\n",
    "            gain_name (str): The name of the gain method used ('entropy' or 'gini').\n",
    "            feature_names (List[str]): List of feature names corresponding to the features in the dataset.\n",
    "            max_depth (Optional[int]): The maximum depth of the tree.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.root: Optional[Node] = None\n",
    "        self.feature_importances_: Optional[np.ndarray] = None\n",
    "        self.feature_names = feature_names\n",
    "        self.gain_name = gain_name\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "            Fit the decision tree classifier to the data.\n",
    "\n",
    "            Parameters:\n",
    "            X (np.ndarray): Feature dataset.\n",
    "            y (np.ndarray): Target values.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.feature_importances_ = np.zeros(self.n_features_)\n",
    "        self.root = self._build_tree(X, y, 0)\n",
    "        total_gain = np.sum(self.feature_importances_)\n",
    "        if total_gain > 0:\n",
    "            self.feature_importances_ /= total_gain\n",
    "\n",
    "            \n",
    "    # Problem 4: Implement the _calculate_gain function\n",
    "    # - Calculate the specified gain (information gain or Gini gain) for a given split.\n",
    "    # - Use the entropy or gini function to calculate the impurity of the parent and child nodes.\n",
    "    # - Return the gain, which is the reduction in impurity.\n",
    "    # Hint: Be careful with division by zero when calculating the weighted average of child impurities\n",
    "\n",
    "    def _calculate_gain(self, y: np.ndarray, feature_column: np.ndarray, split_thresh: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the gain for a particular split.\n",
    "\n",
    "        Parameters:\n",
    "            y (np.ndarray): Target values.\n",
    "            feature_column (np.ndarray): Feature column by which the split is to be made.\n",
    "            split_thresh (float): The threshold value for the split.\n",
    "\n",
    "        Returns:\n",
    "            float: The calculated gain from making the split.\n",
    "        \"\"\"\n",
    "        gain_func = entropy if self.gain_name == 'entropy' else gini\n",
    "        # Calculate the impurity of the parent node\n",
    "        parent_impurity = gain_func(y)\n",
    "        \n",
    "        # Create indices for left and right splits\n",
    "        left_idxs = feature_column <= split_thresh\n",
    "        right_idxs = feature_column > split_thresh\n",
    "        \n",
    "        # Calculate the impurity for each child node\n",
    "        left_impurity = gain_func(y[left_idxs])\n",
    "        right_impurity = gain_func(y[right_idxs])\n",
    "\n",
    "        # TODO: Calculate the weighted average of the child impurities\n",
    "        # START YOUR CODE HERE\n",
    "        left_weight = np.sum(y[left_idxs]) / len(y)\n",
    "        right_weight = np.sum(y[right_idxs]) / len(y)\n",
    "        child_impurity = (left_weight * left_impurity) + (right_weight * right_impurity)\n",
    "        # END YOUR CODE HERE\n",
    "        \n",
    "        # Calculate the gain as the reduction in impurity\n",
    "        reduction = parent_impurity - child_impurity\n",
    "        return reduction\n",
    "\n",
    "    def _best_split(self, X: np.ndarray, y: np.ndarray, num_features: int) -> Tuple[Optional[int], Optional[float], float]:\n",
    "        \"\"\"\n",
    "        Determine the best split for a given dataset.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The dataset.\n",
    "            y (np.ndarray): The target values.\n",
    "            num_features (int): Number of features in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Optional[int], Optional[float], float]: The index of the best feature to split on, the threshold value for the split, and the gain achieved by this split.\n",
    "        \"\"\"\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feature_idx in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._calculate_gain(y, X[:, feature_idx], threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feature_idx\n",
    "                    split_thresh = threshold\n",
    "        return split_idx, split_thresh, best_gain\n",
    "    \n",
    "    def _split(self, feature_column, threshold):\n",
    "        left_idxs = np.where(feature_column <= threshold)[0]\n",
    "        right_idxs = np.where(feature_column > threshold)[0]\n",
    "        return left_idxs, right_idxs   \n",
    "    \n",
    "    # Problem 5: Implement the _build_tree function\n",
    "    # - Complete the _build_tree function to build the decision tree recursively.\n",
    "    # - Use the _best_split function to find the best split for each node.\n",
    "    # - Create left and right subtrees by calling _build_tree recursively.\n",
    "    # Hint: You can use the _split function to split the data based on the best split.\n",
    "    def _build_tree(self, X: np.ndarray, y: np.ndarray, depth: int = 0) -> Node:\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The dataset.\n",
    "            y (np.ndarray): The target values.\n",
    "            depth (int): Current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "            Node: The node that represents the current subtree.\n",
    "        \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        # Base case: If there are no samples or maximum depth is reached, return a leaf node with the most common label\n",
    "        if num_samples == 0 or depth == self.max_depth:\n",
    "            most_common_label = np.bincount(y).argmax() if len(y) > 0 else None\n",
    "            return Node(value=most_common_label)\n",
    "\n",
    "        # Find the best split for the current node using the _best_split function\n",
    "        best_feature, best_thresh, best_gain = self._best_split(X, y, num_features)\n",
    "        \n",
    "        # If no best split is found, return a leaf node with the most common label\n",
    "        if best_feature is None:\n",
    "            most_common_label = np.bincount(y).argmax() if len(y) > 0 else None\n",
    "            return Node(value=most_common_label)\n",
    "        \n",
    "        # Update the feature importances based on the gain of the best split\n",
    "        self.feature_importances_[best_feature] += best_gain\n",
    "        \n",
    "        # START YOUR CODE HERE\n",
    "        # TODO: Split the data into left and right subsets based on the best split threshold\n",
    "        left_indices = X[:, best_feature] <= best_thresh\n",
    "        right_indices = ~left_indices\n",
    "        \n",
    "        left_X, left_y = X[left_indices], y[left_indices]\n",
    "        right_X, right_y = X[right_indices], y[right_indices]\n",
    "        \n",
    "        # Recursively build the left and right subtrees\n",
    "        left = self._build_tree(left_X, left_y, depth + 1)\n",
    "        right = self._build_tree(right_X, right_y, depth + 1)\n",
    "        # END YOUR CODE HERE\n",
    "        \n",
    "        # Create a new internal node with the best split feature, threshold, and the left and right subtrees\n",
    "        return Node(best_feature, best_thresh, left, right, feature_name=self.feature_names[best_feature])\n",
    "\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the class labels for the provided data.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): Dataset for which to predict the class labels.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted class labels.\n",
    "        \"\"\"\n",
    "        return np.array([self._predict(inputs) for inputs in X])\n",
    "\n",
    "    def _predict(self, inputs: np.ndarray) -> Any:\n",
    "        \"\"\"\n",
    "        Helper function to predict the class label for a single data point.\n",
    "\n",
    "        Parameters:\n",
    "            inputs (np.ndarray): The features of the data point.\n",
    "\n",
    "        Returns:\n",
    "            Any: The predicted class label.\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        while node.value is None:\n",
    "            if node.feature is not None and inputs[node.feature] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "            if node is None:  # Safeguard against null node\n",
    "                return 1000  # Or another default value or heuristic\n",
    "        return node.value\n",
    "\n",
    "    def visualize(self, node: Optional[Node] = None, depth: int = 0, parent: Optional[Node] = None, edge_label: str = \"\") -> graphviz.Digraph:\n",
    "        \"\"\"\n",
    "        Visualize the decision tree using Graphviz.\n",
    "\n",
    "        Parameters:\n",
    "            node (Optional[Node]): The current node to visualize. If None, start from the root.\n",
    "            depth (int): The current depth in the tree.\n",
    "            parent (Optional[Node]): The parent node in the tree.\n",
    "            edge_label (str): The label for the edge connecting the current node to its parent.\n",
    "\n",
    "        Returns:\n",
    "            graphviz.Digraph: The Graphviz object representing the decision tree.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "            self.graph = graphviz.Digraph(comment='Decision Tree', format='png')\n",
    "\n",
    "        if node.value is not None:\n",
    "            node_label = f\"Class {node.value}\"\n",
    "        else:\n",
    "            if node.feature is not None and node.feature < len(self.feature_importances_):\n",
    "                gain = self.feature_importances_[node.feature]\n",
    "                if isinstance(gain, np.ndarray) and gain.size == 1:\n",
    "                    gain = gain.item()  # Convert numpy array to scalar if it's a single-element array\n",
    "                else:\n",
    "                    gain = float(gain)  # Ensure it is a scalar\n",
    "                node_label = f\"{node.feature_name} <= {node.threshold}\\nGain: {gain:.3f}\"\n",
    "            else:\n",
    "                node_label = \"Empty\"\n",
    "\n",
    "        self.graph.node(str(node), label=node_label)\n",
    "\n",
    "        if parent:\n",
    "            self.graph.edge(str(parent), str(node), label=edge_label)\n",
    "\n",
    "        if node.left:\n",
    "            self.visualize(node.left, depth + 1, node, \"True\")\n",
    "        if node.right:\n",
    "            self.visualize(node.right, depth + 1, node, \"False\")\n",
    "\n",
    "        if depth == 0:\n",
    "            return self.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cases\n",
    "\n",
    "To verify your implementation, you can run the following cell to compare your decision tree with the sklearn's implementation. **They shall be the same**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "tree = MyDecisionTreeClassifier(max_depth=1, gain_name='gini', feature_names=['X1', 'X2'])\n",
    "tree.fit(X, y)\n",
    "assert tree.predict(np.array([[0, 0]])) == [0], \"Decision Tree prediction test case failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the penguin dataset\n",
    "data = sns.load_dataset('penguins')\n",
    "data.dropna(inplace=True)  # Remove rows with missing values\n",
    "\n",
    "# One-hot encode categorical data\n",
    "data = pd.get_dummies(data, columns=['sex', 'species'], prefix_sep=':-:')\n",
    "y = np.array(LabelEncoder().fit_transform(data['island']))  # Assuming 'species' is the target\n",
    "\n",
    "data = data.drop('island', axis=1)\n",
    "\n",
    "# Select features and target\n",
    "X = np.array(data)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree with gini reduction')\n",
    "\n",
    "for depth in np.arange(1, 8):\n",
    "    print(f\"Depth: {depth}\")\n",
    "    # Initialize and train the classifier\n",
    "    tree_model = MyDecisionTreeClassifier(max_depth=depth, gain_name='gini', feature_names=list(data.columns))\n",
    "    tree_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    y_pred = tree_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of My Tree: {accuracy:.2f}\")\n",
    "\n",
    "    sklearn_tree_model = DecisionTreeClassifier(max_depth=depth, criterion='gini')\n",
    "    sklearn_tree_model.fit(X_train, y_train)\n",
    "    y_pred = sklearn_tree_model.predict(X_test)\n",
    "    print(f\"Accuracy of Sklearn Tree: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree with information gain')\n",
    "\n",
    "for depth in np.arange(1, 8):\n",
    "    print(f\"Depth: {depth}\")\n",
    "    # Initialize and train the classifier\n",
    "    tree_model = MyDecisionTreeClassifier(max_depth=depth, gain_name='entropy', feature_names=list(data.columns))\n",
    "    tree_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    y_pred = tree_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of My Tree: {accuracy:.2f}\")\n",
    "\n",
    "    sklearn_tree_model = DecisionTreeClassifier(max_depth=depth, criterion='entropy')\n",
    "    sklearn_tree_model.fit(X_train, y_train)\n",
    "    y_pred = sklearn_tree_model.predict(X_test)\n",
    "    print(f\"Accuracy of Sklearn Tree: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the feature importances\n",
    "feature_importances = tree_model.feature_importances_\n",
    "feature_names = list(data.columns)\n",
    "importance_dict = dict(zip(feature_names, feature_importances))\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "for feature, importance in importance_dict.items():\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = tree_model.visualize()\n",
    "\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: explain in our implementation, how we calculate feature importance and why?**\n",
    "\n",
    "**[TODO: Write your observations here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance is determined by the differing impurity metrics from one node to another. If the impurity (measured either by entropy or gini index) is significantly lower in a following node, that would imply that the model got that much more \"information\" from that node split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6 (10 points)\n",
    "\n",
    "Implement the bootstrap sampling and feature subsampling steps for random forest models in the `fit` function.\n",
    "\n",
    "- Perform bootstrap sampling by randomly selecting samples with replacement from the training data.\n",
    "- Perform feature subsampling by randomly selecting features without replacement.\n",
    "- Create and train a decision tree classifier using the bootstrap sample and selected features.\n",
    "- Append the trained tree and its selected feature indices to the list of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7 (10 points)\n",
    "\n",
    "Implement the `predict` function.\n",
    "\n",
    "- Make predictions using each trained tree with its selected features.\n",
    "- Store the predictions from each tree in the predictions array.\n",
    "- Perform majority voting to determine the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "from sympy import N\n",
    "\n",
    "class MyRandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_features='sqrt', max_depth=None, random_state=None, feature_names=None):\n",
    "        self.n_estimators = n_estimators  # Number of trees in the forest\n",
    "        self.max_features = max_features  # The number of features to consider when looking for the best split\n",
    "        self.max_depth = max_depth  # Maximum depth of the tree\n",
    "        self.random_state = random_state  # Random seed\n",
    "        self.feature_names = np.array(feature_names)\n",
    "        self.trees = []  # List to store individual trees\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Determine the number of features to consider for each split\n",
    "        if self.max_features == 'sqrt':\n",
    "            self.max_features = int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            self.max_features = int(np.log2(n_features))\n",
    "        else:\n",
    "            self.max_features = n_features\n",
    "\n",
    "        # Set the random seed for reproducibility\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Problem 6: Implement the bootstrap sampling and feature subsampling steps\n",
    "            # - Perform bootstrap sampling by randomly selecting samples with replacement from the training data\n",
    "            # - Perform feature subsampling by randomly selecting features without replacement\n",
    "            # - Create and train a decision tree classifier using the bootstrap sample and selected features\n",
    "            # - Append the trained tree and its selected feature indices to the list of trees\n",
    "            # Hint: Use np.random.choice to perform random sampling\n",
    "            \n",
    "            # START YOUR CODE HERE\n",
    "            # TODO: Perform bootstrap sampling by randomly selecting samples with replacement from the training data\n",
    "            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            # indices = np.random.choice(n_samples, len(n_samples), True)\n",
    "            # END YOUR CODE HERE\n",
    "\n",
    "            X_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "\n",
    "            # START YOUR CODE HERE\n",
    "            # TODO: Perform feature subsampling by randomly selecting features without replacement\n",
    "            features_indices = np.random.choice(n_features, size=n_features, replace=False)\n",
    "            # END YOUR CODE HERE\n",
    "\n",
    "            X_train_sampled = X_sample[:, features_indices]\n",
    "\n",
    "            # Create and train a decision tree classifier using the bootstrap sample and selected features\n",
    "            clf = MyDecisionTreeClassifier(max_depth=self.max_depth, gain_name='gini', feature_names=self.feature_names[features_indices])\n",
    "            clf.fit(X_train_sampled, y_sample)\n",
    "            self.trees.append((clf, features_indices))\n",
    "            # END YOUR CODE HERE\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], len(self.trees)), dtype=int)\n",
    "\n",
    "        # Problem 7: Implement the prediction step\n",
    "        # - Make predictions using each trained tree with its selected features\n",
    "        # - Store the predictions from each tree in the predictions array\n",
    "        # - Perform majority voting to determine the final predictions\n",
    "        # Hint: Use the mode function from scipy.stats for majority voting\n",
    "        \n",
    "        # START YOUR CODE HERE\n",
    "        for i, (tree, features_indices) in enumerate(self.trees):\n",
    "            # TODO: Make predictions using each trained tree with its selected features\n",
    "            predictions[:, i] = tree.predict(X[:, features_indices])\n",
    "\n",
    "        # TODO: Perform majority voting to determine the final predictions\n",
    "        final_predictions, _ = mode(predictions, axis=1)\n",
    "        # END YOUR CODE HERE\n",
    "        \n",
    "        return final_predictions.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cases\n",
    "\n",
    "Now you can run the following cell to compare your random forest with the sklearn's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case for Random Forest\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "rf = MyRandomForestClassifier(n_estimators=2, max_depth=1, feature_names=['X1', 'X2'])\n",
    "rf.fit(X, y)\n",
    "assert rf.predict(np.array([[0, 0]])) == [0], \"Random Forest prediction test case failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for n_estimators in [5, 10, 20, 50]:\n",
    "    forest = MyRandomForestClassifier(max_depth=3, n_estimators=n_estimators, feature_names=list(data.columns))\n",
    "    forest.fit(X_train, y_train)\n",
    "    # Evaluate the classifier\n",
    "    y_pred = forest.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of My Forest: {n_estimators:.2f} {accuracy:.2f}\")\n",
    "\n",
    "    forest = RandomForestClassifier(max_depth=3, n_estimators=n_estimators)\n",
    "    forest.fit(X_train, y_train)\n",
    "    # Evaluate the classifier\n",
    "    y_pred = forest.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy of sklearn Forest: {n_estimators:.2f} {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Mixture of Experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8 (10 points)\n",
    "\n",
    "Implement the `GatingNetwork` class.\n",
    "Its purpose is to learn and assign weights to each expert based on the input features. The GatingNetwork takes the same input as the experts and outputs a set of weights, one for each expert. These weights determine the contribution of each expert to the final output of the Mixture of Experts model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9 (10 points)\n",
    "\n",
    "Implement the `MixtureOfExperts` class.\n",
    "It combines multiple expert networks and a gating network to make predictions. The main idea is to divide the task among several specialized expert networks and let the gating network determine the contribution of each expert to the final output based on the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Expert, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_experts):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        # START YOUR CODE HERE\n",
    "        # Implement the GatingNetwork class with the following specifications:\n",
    "        # - The network should have one linear layer with input_size units and num_experts units.\n",
    "        # - Use nn.Linear to define the layer.\n",
    "        self.linear = nn.Linear(input_size, num_experts)\n",
    "        # END YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        # START YOUR CODE HERE\n",
    "        # Implement the forward pass of the GatingNetwork class.\n",
    "        # - Pass the input tensor through the linear layer defined in the constructor.\n",
    "        # - Apply the softmax activation function to the output of the linear layer.\n",
    "        # - Use F.softmax with dim=1 to apply softmax along the second dimension.\n",
    "        output = self.linear(x)\n",
    "        return F.softmax(output, dim=1)\n",
    "        # END YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case for GatingNetwork\n",
    "input_size = 4\n",
    "num_experts = 3\n",
    "gating_network = GatingNetwork(input_size, num_experts)\n",
    "x = torch.randn(2, input_size)\n",
    "gates = gating_network(x)\n",
    "assert gates.shape == (2, num_experts), \"GatingNetwork output shape test case failed.\"\n",
    "assert torch.allclose(gates.sum(dim=1), torch.tensor([1.0, 1.0])), \"GatingNetwork output sum test case failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, num_experts, input_size, hidden_size, output_size):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_size, hidden_size, output_size) for _ in range(num_experts)])\n",
    "        self.gating_network = GatingNetwork(input_size, num_experts)\n",
    "\n",
    "    def get_gate(self, x):\n",
    "        return self.gating_network(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get weights for each expert from the gating network\n",
    "        gates = self.gating_network(x)\n",
    "\n",
    "        # Get outputs from each expert\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "\n",
    "        # Combine outputs from all experts\n",
    "        # START YOUR CODE HERE\n",
    "        # TODO: Implement the combination of expert outputs with the following steps:\n",
    "        # - Convert the list of expert outputs to a tensor using torch.stack.\n",
    "        #   The shape of the tensor should be [batch_size, num_experts, output_size].\n",
    "        expert_outputs_tensor = torch.stack(expert_outputs, dim=1)\n",
    "        # - Multiply the expert outputs tensor with the gating weights (unsqueezed to add an extra dimension).\n",
    "        weighted_outputs = expert_outputs_tensor * gates.unsqueeze(-1)\n",
    "        # - Sum the multiplied outputs along the second dimension (dim=1) to get the final output.\n",
    "        #   The shape of the final output should be [batch_size, output_size].\n",
    "        result = weighted_outputs.sum(dim=1)\n",
    "\n",
    "        # END YOUR CODE HERE\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case for MixtureOfExperts\n",
    "input_size = 4\n",
    "hidden_size = 8\n",
    "output_size = 2\n",
    "num_experts = 3\n",
    "moe = MixtureOfExperts(num_experts, input_size, hidden_size, output_size)\n",
    "x = torch.randn(2, input_size)\n",
    "output = moe(x)\n",
    "assert output.shape == (2, output_size), \"MixtureOfExperts output shape test case failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values\n",
    "X = np.linspace(-2, 2, 1000)\n",
    "# Compute y values based on the given conditions\n",
    "y = np.array([x if x > 0 else (x**3 + 5) for x in X]).reshape([-1,1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape([-1,1]), y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "input_size = 1\n",
    "hidden_size = 128\n",
    "output_size = 5\n",
    "num_experts = 3\n",
    "\n",
    "model = MixtureOfExperts(num_experts, input_size, hidden_size, output_size)\n",
    "input_tensor = torch.randn(2, input_size)\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_t = torch.FloatTensor(X_train.astype(float))\n",
    "X_test_t = torch.FloatTensor(X_test.astype(float))\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create DataLoader instances for batching\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "# Define the Mixture of Experts model\n",
    "input_size = X_train.shape[1]\n",
    "output_size = 1\n",
    "num_experts = 2  # Number of experts\n",
    "hidden_size = 32  # Size of hidden layer in experts and gating network\n",
    "\n",
    "model = MixtureOfExperts(num_experts, input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    eval_losses.append(total_loss / len(test_loader))\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch+1}, Eval Loss: {total_loss / len(test_loader): .4f}')\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader): .4f}')\n",
    "    evaluate(model, test_loader)\n",
    "\n",
    "# Plot the loss and accuracy\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(train_losses, label='Training Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(eval_losses, label='Evaluation Loss', color='orange')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gates_prob = model.gating_network(torch.FloatTensor(X).view([-1,1])).detach().numpy()\n",
    "\n",
    "plt.plot(gates_prob[:,0])\n",
    "plt.plot(gates_prob[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:,0], model(X_train_t).reshape(-1).detach().numpy())\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10 (10 points)\n",
    "Compare the performance of the Mixture of Experts (MoE) model and a Sparse Mixture of Experts (SMoE) model.\n",
    "Instructions:\n",
    "- Implement a Sparse Mixture of Experts (SMoE) model based on the paper \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\" (https://arxiv.org/abs/1701.06538, Section 2.1).\n",
    "- Modify the Mixture of Experts (MoE) model to include the sparsity parameter `k`.\n",
    "- Train and evaluate both the MoE and SMoE models using the same training loop and evaluation function.\n",
    "- Compare the performance of the MoE and SMoE models.\n",
    "- Write your observations in the comments below.\n",
    "\n",
    "Hints:\n",
    "- In the SMoE model, during training, add tunable Gaussian noise to the gating weights before applying the softmax function.\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\mathbf{n} & = \\mathbf{x} \\mathbf{W}_n, \\\\\n",
    "    \\mathbf{\\sigma} & = \\text{softplus}(\\mathbf{n}) + \\epsilon, \\\\\n",
    "    \\mathbf{l} & = \\mathbf{c} + \\mathcal{N}(0, \\mathbf{\\sigma}^2),\n",
    "    \\end{align}\n",
    "    $$\n",
    "    where $\\mathbf{n}$ is the vector of raw noise standard deviations, $\\mathbf{W}_n$ is the weight matrix for the noise gate, $\\mathbf{\\sigma}$ is the vector of noise standard deviations, $\\epsilon$ is a small constant for numerical stability, and $\\mathbf{l}$ is the vector of noisy logits.\n",
    "- The top-k gating mechanism selects the k experts with the highest noisy logits:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\mathbf{t}, \\mathbf{i} & = \\text{topk}(\\mathbf{l}, k+1), \\\\\n",
    "    \\mathbf{t}_k & = \\mathbf{t}_{:k}, \\\\\n",
    "    \\mathbf{i}_k & = \\mathbf{i}_{:k},\n",
    "    \\end{align}\n",
    "    $$\n",
    "    where $\\mathbf{t}$ is the vector of top-k+1 logits, $\\mathbf{i}$ is the vector of top-k+1 indices, $\\mathbf{t}_k$ is the vector of top-k logits, and $\\mathbf{i}_k$ is the vector of top-k indices.\n",
    "\n",
    "- The gating weights are obtained by applying the softmax function to the top-k logits:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\mathbf{g}_k & = \\text{softmax}(\\mathbf{t}_k), \\\\\n",
    "    \\mathbf{g} & = \\text{scatter}(\\mathbf{0}, \\mathbf{i}_k, \\mathbf{g}_k),\n",
    "    \\end{align}\n",
    "    $$\n",
    "    where $\\mathbf{g}_k$ is the vector of top-k gating weights, and $\\mathbf{g}$ is the sparse vector of gating weights with non-zero values at the top-k indices.\n",
    "\n",
    "When writing your observations, remember to include the following:\n",
    "- Compare the training and evaluation results of the MoE and SMoE models.\n",
    "- Discuss any differences in performance, convergence, or computational efficiency between the two models.\n",
    "- Provide insights on how the sparsity and noisy gating mechanisms in the SMoE model affect its performance compared to the standard MoE model.\n",
    "- Analyze the impact of the sparsity parameter k on the performance and efficiency of the SMoE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseGatingNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_experts, k, noise_epsilon=1e-3):\n",
    "        super(SparseGatingNetwork, self).__init__()\n",
    "        self.gate = nn.Linear(input_size, num_experts)\n",
    "        self.noise_gate = nn.Linear(input_size, num_experts)\n",
    "        self.k = k\n",
    "        self.noise_epsilon = noise_epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Problem 10: Implement Noisy Top-K Gating\n",
    "        # - Add tunable Gaussian noise to the gating weights before applying the softmax function.\n",
    "        # - Keep only the top k values, setting the rest to -∞.\n",
    "        # - Use the provided equations as a reference.\n",
    "        # Hints:\n",
    "        # - Use torch.randn_like to generate standard normal noise with the same shape as the gating weights.\n",
    "        # - Use F.softplus to apply the softplus function to the noise weights.\n",
    "        # - Use torch.topk to find the top k values and their indices.\n",
    "        # - Create a new tensor with -∞ values and update the top k values using the indices.\n",
    "\n",
    "        # START YOUR CODE HERE\n",
    "        gating_weights = self.gate(x)\n",
    "        noise_weights = self.noise_gate(x)\n",
    "\n",
    "        # Add tunable Gaussian noise to the gating weights\n",
    "        noisy_gating_weights = gating_weights + torch.randn_like(gating_weights) * F.softplus(noise_weights + self.noise_epsilon)\n",
    "\n",
    "        # Get the top-k values and their indices\n",
    "        top_values, top_indices = torch.topk(noisy_gating_weights, self.k, dim=1)\n",
    "\n",
    "        # Create a new tensor with -inf values\n",
    "        gates = torch.full_like(noisy_gating_weights, float('-inf'))\n",
    "\n",
    "        # Update the top-k values in the new tensor\n",
    "        gates = gates.scatter(1, top_indices, top_values)\n",
    "\n",
    "        # Apply softmax to get the final gating weights\n",
    "        gates = F.softmax(gates, dim=1)\n",
    "        # END YOUR CODE HERE\n",
    "        return gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, num_experts, input_size, hidden_size, output_size, k, noisy_gating=True):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_size, hidden_size, output_size) for _ in range(num_experts)])\n",
    "        self.gating_network = SparseGatingNetwork(input_size, num_experts, k, noisy_gating)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # START YOUR CODE HERE\n",
    "        # TODO: Copy your code from Problem 9\n",
    "        output = self.gating_network(x)\n",
    "        return F.softmax(output, dim=1)\n",
    "        # END YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case for SparseGatingNetwork\n",
    "input_size = 4\n",
    "num_experts = 5\n",
    "k = 3\n",
    "sparse_gating_network = SparseGatingNetwork(input_size, num_experts, k)\n",
    "x = torch.randn(2, input_size)\n",
    "gates = sparse_gating_network(x)\n",
    "assert gates.shape == (2, num_experts), \"SparseGatingNetwork output shape test case failed.\"\n",
    "assert torch.allclose(gates.sum(dim=1), torch.tensor([1.0, 1.0])), \"SparseGatingNetwork output sum test case failed.\"\n",
    "assert (gates > 0).sum(dim=1).max() <= k, \"SparseGatingNetwork top-k test case failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experts = 5\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 8\n",
    "k = 3\n",
    "\n",
    "model = MixtureOfExperts(num_experts, input_size, hidden_size, output_size, k)\n",
    "\n",
    "batch_size = 4\n",
    "x = torch.randn(batch_size, input_size)\n",
    "\n",
    "output = model(x)\n",
    "print(output.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your observations here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
